# -*- coding: utf-8 -*-
"""paper_calc_NSI_long_term

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fqcb5r2v36AoSPiujPwUY2UVeapvXe_9

### Import library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set( style="whitegrid" )

import math

"""###. Load prompting data, K-NSI"""

news = pd.read_csv( "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/merged_temperature_0.2_C1.csv" )

# news

news["date"] = pd.to_datetime( news["date"] )

news["month"] = news["date"].dt.month

"""###. Calc NSI"""

NSI = pd.DataFrame( columns=["month", "date", "positive", "negative", "neutral", "Xt", "Xbar", "T", "S", "distance"] )
NSI["date"] = news["date"].unique()

NSI["date"] = pd.to_datetime( NSI["date"] )
NSI["month"] = NSI["date"].dt.month

def calc_NSI_T( ptype, news, NSI ):

  T = 0
  for m in news["month"].unique():

    news_m = news[news["month"]==m]

    positive_m = 0
    negative_m = 0
    neutral_m = 0

    for d in news_m["date"].unique():
      # print( d )

      daily = news_m[news_m["date"]==d]
      group = daily.groupby(ptype)[ptype].count()

      try:
        positive = group['positive']
      except KeyError:
        positive = 0

      try:
        negative = group['negative']
      except KeyError:
        negative = 0

      try:
        neutral = group['neutral']
      except KeyError:
        neutral = 0

      NSI.loc[NSI["date"]==d, "positive"] = positive
      NSI.loc[NSI["date"]==d, "negative"] = negative
      NSI.loc[NSI["date"]==d, "neutral"] = neutral

      positive_m = positive_m + positive
      negative_m = negative_m + negative
      neutral_m = neutral_m + neutral

      T += 1
      NSI.loc[NSI["date"]==d, "T"] = T

    NSI.loc[NSI["month"]==m, "positive_m"] = positive_m
    NSI.loc[NSI["month"]==m, "negative_m"] = negative_m
    NSI.loc[NSI["month"]==m, "neutral_m"] = neutral_m

  return NSI

def calc_NSI_Xt( NSI ):

  for i in range( 0, len(NSI) ):

    positive_sum = 0
    negative_sum = 0

    if i-5 < 0:
      positive_sum = NSI.loc[0:i,"positive"].sum()
      negative_sum = NSI.loc[0:i, "negative"].sum()
    else:
      positive_sum = NSI.loc[i-5:i, "positive"].sum()
      negative_sum = NSI.loc[i-5:i, "negative"].sum()

    # print( positive_sum, negative_sum )

    if (positive_sum - negative_sum) == 0:
      NSI.loc[i, "Xt"] = (positive_sum - negative_sum) / 1
    else:
      NSI.loc[i, "Xt"] = (positive_sum - negative_sum) / (positive_sum + negative_sum)


    # print( i, " : ", NSI.loc[i, "Xt"] )

  return NSI

def calc_NSI_Xt_m( NSI ):


  NSI["Xt_m"] = (NSI["positive_m"] - NSI["negative_m"]) / (NSI["positive_m"] + NSI["negative_m"])

  return NSI

def calc_NSI_Xbar( NSI ):

  for i in range( 0, len(NSI) ):

    Xt_sum = 0
    duration = 365

    if i-duration < 0:
      Xt_sum = NSI.loc[0:i,"Xt"].sum()
    else:
      Xt_sum = NSI.loc[i-duration:i, "Xt"].sum()

    NSI.loc[i, "Xbar"] = Xt_sum / duration

  return NSI

def calc_NSI_Xbar_m( NSI ):

  for m in range( 0, len(NSI["month"].unique()) ):

    Xt_sum = 0
    duration = 1

    if i-duration < 0:
      Xt_sum = NSI.loc[0:i,"Xt_m"].sum()
    else:
      Xt_sum = NSI.loc[i-duration:i, "Xt_m"].sum()

    NSI.loc[i, "Xbar_m"] = Xt_sum / 1

  return NSI

def calc_NSI_S( NSI ):


  for i in range( 0, NSI["date"].count() ):
    duration = 364
    distance = (NSI.loc[i, "Xt"] - NSI.loc[i, "Xbar"]) ** 2
    NSI.loc[i, "distance"] = distance

    distance_sum = NSI.loc[0:i, "distance"].sum()

    S = math.sqrt( distance_sum / duration )

    NSI.loc[i, "S"] = S

    # print( i, " : ", S)
  return NSI

def calc_NSI_S_m( NSI ):


  for i in range( 0, NSI["date"].count() ):
    duration = 364
    distance = (NSI.loc[i, "Xt"] - NSI.loc[i, "Xbar"]) ** 2
    NSI.loc[i, "distance"] = distance

    distance_sum = NSI.loc[0:i, "distance"].sum()

    S = math.sqrt( distance_sum / duration )

    NSI.loc[i, "S"] = S

    # print( i, " : ", S)
  return NSI

def calc_NSI( ptype, news, NSI ):

  NSI = calc_NSI_T( ptype, news, NSI )
  NSI = calc_NSI_Xt( NSI )
  NSI = calc_NSI_Xt_m( NSI )
  NSI = calc_NSI_Xbar( NSI )
  NSI = calc_NSI_S( NSI )

  column_name = ptype + "_P-NSI"
  column_name_m = column_name + "_m"

  # NSI[column_name] = ( ( (NSI["Xt"] - NSI["Xbar"]) / (NSI["S"]+1) ) * 10 ) + 100
  NSI[column_name] = ( NSI["Xt"] * 10 ) + 100
  NSI[column_name_m] = ( NSI["Xt_m"] * 10 ) + 100

  # print( NSI["Xt_m"] )

  return NSI



GPT_P1_NSI = calc_NSI( "GPT_P1", news, NSI )
GPT_P2_NSI = calc_NSI( "GPT_P2", news, NSI )
GPT_P3_NSI = calc_NSI( "GPT_P3", news, NSI )
GPT_P4_NSI = calc_NSI( "GPT_P4", news, NSI )

GPT_P1C_NSI = calc_NSI( "GPT_P1C", news, NSI )
GPT_P2C_NSI = calc_NSI( "GPT_P2C", news, NSI )
GPT_P3C_NSI = calc_NSI( "GPT_P3C", news, NSI )
GPT_P4C_NSI = calc_NSI( "GPT_P4C", news, NSI )

GPT_P1N_NSI = calc_NSI( "GPT_P1N_str", news, NSI )
GPT_P2N_NSI = calc_NSI( "GPT_P2N_str", news, NSI )
GPT_P3N_NSI = calc_NSI( "GPT_P3N_str", news, NSI )
GPT_P4N_NSI = calc_NSI( "GPT_P4N_str", news, NSI )

GPT_P1NC_NSI = calc_NSI( "GPT_P1NC_str", news, NSI )
GPT_P2NC_NSI = calc_NSI( "GPT_P2NC_str", news, NSI )
GPT_P3NC_NSI = calc_NSI( "GPT_P3NC_str", news, NSI )
GPT_P4NC_NSI = calc_NSI( "GPT_P4NC_str", news, NSI )

NSI = pd.DataFrame()



NSI = pd.merge( GPT_P1_NSI[["date", "month", "GPT_P1_P-NSI", "GPT_P1_P-NSI_m"]], GPT_P2_NSI[["date", "GPT_P2_P-NSI", "GPT_P2_P-NSI_m"]], on="date", how="inner")
NSI = pd.merge( NSI, GPT_P3_NSI[["date", "GPT_P3_P-NSI", "GPT_P3_P-NSI_m"]], on="date", how="inner")
NSI = pd.merge( NSI, GPT_P4_NSI[["date", "GPT_P4_P-NSI", "GPT_P4_P-NSI_m"]], on="date", how="inner")

NSI = pd.merge( NSI, GPT_P1C_NSI[["date", "GPT_P1C_P-NSI", "GPT_P1C_P-NSI_m"]], on="date", how="inner")
NSI = pd.merge( NSI, GPT_P2C_NSI[["date", "GPT_P2C_P-NSI", "GPT_P2C_P-NSI_m"]], on="date", how="inner")
NSI = pd.merge( NSI, GPT_P3C_NSI[["date", "GPT_P3C_P-NSI", "GPT_P3C_P-NSI_m"]], on="date", how="inner")
NSI = pd.merge( NSI, GPT_P4C_NSI[["date", "GPT_P4C_P-NSI", "GPT_P4C_P-NSI_m"]], on="date", how="inner")

NSI = pd.merge( NSI, GPT_P1N_NSI[["date", "GPT_P1N_str_P-NSI", "GPT_P1N_str_P-NSI_m"]], on="date", how="inner")
NSI = pd.merge( NSI, GPT_P2N_NSI[["date", "GPT_P2N_str_P-NSI", "GPT_P2N_str_P-NSI_m"]], on="date", how="inner")
NSI = pd.merge( NSI, GPT_P3N_NSI[["date", "GPT_P3N_str_P-NSI", "GPT_P3N_str_P-NSI_m"]], on="date", how="inner")
NSI = pd.merge( NSI, GPT_P4N_NSI[["date", "GPT_P4N_str_P-NSI", "GPT_P4N_str_P-NSI_m"]], on="date", how="inner")

NSI = pd.merge( NSI, GPT_P1N_NSI[["date", "GPT_P1NC_str_P-NSI", "GPT_P1NC_str_P-NSI_m"]], on="date", how="inner")
NSI = pd.merge( NSI, GPT_P2N_NSI[["date", "GPT_P2NC_str_P-NSI", "GPT_P2NC_str_P-NSI_m"]], on="date", how="inner")
NSI = pd.merge( NSI, GPT_P3N_NSI[["date", "GPT_P3NC_str_P-NSI", "GPT_P3NC_str_P-NSI_m"]], on="date", how="inner")
NSI = pd.merge( NSI, GPT_P4N_NSI[["date", "GPT_P4NC_str_P-NSI", "GPT_P4NC_str_P-NSI_m"]], on="date", how="inner")

columns = [ "GPT_P1_P-NSI_m", "GPT_P2_P-NSI_m","GPT_P3_P-NSI_m","GPT_P4_P-NSI_m","GPT_P1C_P-NSI_m","GPT_P2C_P-NSI_m","GPT_P3C_P-NSI_m","GPT_P4C_P-NSI_m","GPT_P1N_str_P-NSI_m","GPT_P2N_str_P-NSI_m","GPT_P3N_str_P-NSI_m","GPT_P4N_str_P-NSI_m", "GPT_P1NC_str_P-NSI_m",
"GPT_P2NC_str_P-NSI_m",
"GPT_P3NC_str_P-NSI_m",
"GPT_P4NC_str_P-NSI_m"]

NSI_m = NSI.groupby("month")[columns].mean().reset_index()

NSI_m_C1 = NSI_m

NSI_m_C2 = NSI_m

NSI_m_C3 = NSI_m

NSI_m_GPT_4_C1 = NSI_m

NSI_m_GPT_4_C2 = NSI_m

NSI_m_GPT_4_C3 = NSI_m

NSI_m_C1["reputation"] = 1
NSI_m_C1["engine"] = "GPT-3.5-Turbo"

NSI_m_C2["reputation"] = 2
NSI_m_C2["engine"] = "GPT-3.5-Turbo"

NSI_m_C3["reputation"] = 3
NSI_m_C3["engine"] = "GPT-3.5-Turbo"

NSI_m_GPT_4_C1["reputation"] = 1
NSI_m_GPT_4_C1["engine"] = "GPT-4-Turbo"

NSI_m_GPT_4_C2["reputation"] = 2
NSI_m_GPT_4_C2["engine"] = "GPT-4-Turbo"

NSI_m_GPT_4_C3["reputation"] = 3
NSI_m_GPT_4_C3["engine"] = "GPT-4-Turbo"

NSI_m = pd.DataFrame( columns=NSI_m_C1.columns )

NSI_m = pd.concat( [NSI_m_C1, NSI_m_C2], axis=0 )
NSI_m = pd.concat( [NSI_m, NSI_m_C3], axis=0 )
NSI_m = pd.concat( [NSI_m, NSI_m_GPT_4_C1], axis=0 )
NSI_m = pd.concat( [NSI_m, NSI_m_GPT_4_C2], axis=0 )
NSI_m = pd.concat( [NSI_m, NSI_m_GPT_4_C3], axis=0 )

NSI_m.to_csv( "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/NSI_m.csv", index=False )

NSI_m

# @title engine

from matplotlib import pyplot as plt
import seaborn as sns
NSI_m.groupby('engine').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)









NSI_m.columns

NSI_m = pd.read_csv( "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/NSI_m.csv" )

for p in ['GPT_P1_P-NSI_m', 'GPT_P2_P-NSI_m', 'GPT_P3_P-NSI_m', 'GPT_P1C_P-NSI_m',
       'GPT_P2C_P-NSI_m', 'GPT_P3C_P-NSI_m',
       'GPT_P1N_str_P-NSI_m', 'GPT_P2N_str_P-NSI_m', 'GPT_P3N_str_P-NSI_m',
       'GPT_P1NC_str_P-NSI_m', 'GPT_P2NC_str_P-NSI_m',
       'GPT_P3NC_str_P-NSI_m']:

  rep1 = NSI_m.loc[(NSI_m["reputation"]==1) & (NSI_m["engine"]=="GPT-3.5-Turbo"), p].reset_index()
  rep2 = NSI_m.loc[(NSI_m["reputation"]==2) & (NSI_m["engine"]=="GPT-3.5-Turbo"), p].reset_index()
  rep3 = NSI_m.loc[(NSI_m["reputation"]==3) & (NSI_m["engine"]=="GPT-3.5-Turbo"), p].reset_index()

  rep4 = NSI_m.loc[(NSI_m["reputation"]==1) & (NSI_m["engine"]=="GPT-4-Turbo"), p].reset_index()
  rep5 = NSI_m.loc[(NSI_m["reputation"]==2) & (NSI_m["engine"]=="GPT-4-Turbo"), p].reset_index()
  rep6 = NSI_m.loc[(NSI_m["reputation"]==3) & (NSI_m["engine"]=="GPT-4-Turbo"), p].reset_index()


  all_data = pd.concat([rep1[[p]], rep2[[p]], rep3[[p]], rep4[[p]], rep5[[p]], rep6[[p]] ], axis=1)
  all_data.columns = all_data.columns.str.replace("_P-NSI_m", "")
  all_data.columns = all_data.columns.str.replace("_str", "")

  correlation_matrix = all_data.corr()

  plt.figure(figsize=(10, 8))
  sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)

  p = p.replace( "_P-NSI_m", "" )
  p = p.replace( "_str", "" )

  plt.title(f'Correlation Matrix for {p}')
  plt.show()







import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
file_path = "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/NSI_m.csv"
df = pd.read_csv(file_path)

# Select the columns for the three time series
time_series_columns = ["GPT_P1_P-NSI_m", "GPT_P2_P-NSI_m", "GPT_P1C_P-NSI_m"]

# Calculate mean and standard deviation for each time series
stats = df[time_series_columns].agg(['mean', 'std']).T
stats.columns = ['Mean', 'Standard Deviation']

# Calculate the correlation matrix
correlation_matrix = df[time_series_columns].corr()

# Plot the trends of the three time series
plt.figure(figsize=(14, 8))

for col in time_series_columns:
    sns.lineplot(x='month', y=col, data=df, marker='o', label=col)

# Adding trend lines
for col in time_series_columns:
    z = np.polyfit(df['month'], df[col], 1)
    p = np.poly1d(z)
    plt.plot(df['month'], p(df['month']), linestyle="--", label=f"{col} Trend")

plt.title('Trends of Selected GPT Prompts Over Months')
plt.xlabel('Month')
plt.ylabel('NSI_m Value')
plt.legend(title='GPT Prompt')
plt.show()

import ace_tools as tools; tools.display_dataframe_to_user(name="Time Series Statistics", dataframe=stats)
import ace_tools as tools; tools.display_dataframe_to_user(name="Correlation Matrix", dataframe=correlation_matrix)

# @title month vs GPT_P1_P-NSI_m

from matplotlib import pyplot as plt
NSI_m.plot(kind='scatter', x='month', y='GPT_P1_P-NSI_m', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

# @title month

from matplotlib import pyplot as plt
NSI_m['month'].plot(kind='hist', bins=20, title='month')
plt.gca().spines[['top', 'right',]].set_visible(False)

# NSI_m

NSI_m = pd.read_csv( "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/NSI_m.csv" )

ECO = pd.read_csv( "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/ECO_indicators_total.csv" )
ECO = ECO.loc[(ECO["통계표"] >= "2023/01") & (ECO["통계표"] <= "2023/12")]
ECO["year"] = ECO["year"].astype(int)
ECO["month"] = ECO["month"].astype(int)

ECO = ECO.drop( columns=["통계표", "year"])



# from statsmodels.tsa.stattools import ccf


# # 교차상관 함수
# cross_corr = ccf(NSI_m["GPT_P1N_str_P-NSI_m"], NSI_m["CCSI"])

# print( cross_corr )

# plt.figure(figsize=(12, 6))
# plt.plot(cross_corr)
# plt.title('Cross-Correlation Function')
# plt.xlabel('Lags')
# plt.ylabel('Correlation')
# plt.show()

NSI_temp = NSI_m.loc[(NSI_m["reputation"]==3) & (NSI_m["engine"]=="GPT-4-Turbo")]
NSI_temp = NSI_temp.drop( columns=["month", "reputation", "engine"] )

from statsmodels.tsa.stattools import ccf

corr_df = pd.DataFrame( columns=["GPT_NSI", "ECO", "max_corr", "max_delay"] )

for g in NSI_temp.columns:

  if g != "GPT_P4_P-NSI_m" and g != "GPT_P4N_str_P-NSI_m" and g != "GPT_P4C_P-NSI_m" and g != "GPT_P4NC_str_P-NSI_m" :

    for e in ECO.columns:
      if g != "month" and e != "month" and e != "통계표" and e != "year":

        # 샘플 데이터 생성
        x = NSI_temp[g]
        y = ECO[e]

        x = x - np.mean(x)
        y = y - np.mean(y)

        # 교차상관 함수
        lags = np.arange(-len(x) + 1, len(x))
        cross_corr = ccf(y, x)

        # print( cross_corr2 )
        max_corr_lag = lags[np.argmax(cross_corr)]
        max_corr_value = np.max(cross_corr)
        # print(f"최대 상관 계수: {max_corr_value}, 시간 지연: {max_corr_lag}")

        corr_data = {"GPT_NSI":g, "ECO":e, "max_corr":max_corr_value, "max_delay":max_corr_lag}
        corr_df.loc[len(corr_df)] = corr_data

len(corr_df)

target_df = pd.DataFrame( columns=["GPT_NSI", "ECO", "max_corr", "max_delay"] )

for e in corr_df["ECO"].unique():
  temp = corr_df.loc[(corr_df["ECO"]== e) & (corr_df["max_corr"] < 1)]

  max_corr = temp["max_corr"].max()
  target = temp.loc[temp["max_corr"]==max_corr]

  target_df.loc[len(target_df)] = target.iloc[0]

target_df_1 = target_df

target_df_2 = target_df

target_df_3 = target_df

target_df_4 = target_df

target_df_5 = target_df

target_df_6 = target_df

ECO_comparision = pd.concat( [target_df_1, target_df_2[["max_corr", "max_delay"]]], axis=1 )
ECO_comparision = pd.concat( [ECO_comparision, target_df_3[["max_corr", "max_delay"]]], axis=1 )
ECO_comparision = pd.concat( [ECO_comparision, target_df_4[["max_corr", "max_delay"]]], axis=1 )
ECO_comparision = pd.concat( [ECO_comparision, target_df_5[["max_corr", "max_delay"]]], axis=1 )
ECO_comparision = pd.concat( [ECO_comparision, target_df_6[["max_corr", "max_delay"]]], axis=1 )

ECO_comparision.to_csv( "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/ECO_comparision.csv", index=False )

ECO_comparision



ECO_comparision = pd.read_csv( "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/ECO_comparision.csv" )













from matplotlib import pyplot as plt
import seaborn as sns
target_df.groupby('max_delay').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

from matplotlib import pyplot as plt
import seaborn as sns
import pandas as pd
plt.subplots(figsize=(8, 8))
df_2dhist = pd.DataFrame({
    x_label: grp['max_delay'].value_counts()
    for x_label, grp in target_df.groupby('GPT_NSI')
})
sns.heatmap(df_2dhist, cmap='viridis')
plt.xlabel('GPT_NSI')
_ = plt.ylabel('max_delay')

from matplotlib import pyplot as plt
import seaborn as sns
target_df.groupby('GPT_NSI').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

for e in corr_df["ECO"].unique():
  temp = corr_df.loc[(corr_df["ECO"]== e) & (corr_df["max_corr"] < 1)]

  max_corr = temp["max_corr"].max()
  target = temp.loc[temp["max_corr"]==max_corr]

  print( target )

  max_corr = max( temp )















NSI_m













K_NSI_total = pd.DataFrame( {"K-NSI":[97.41,96.01	,95.88	,101.83	,94.9	,94.98	,89.41	,83.19	,95.29	,92.9	,104.19	,105.8	,102.15	,81.69	,70.61	,80.02	,90.21	,98.89	,106.24	,99.54	,99.46	,104.95	,113.06	,113.75	,114.42	,107.39	,114.48	,119.96	,115.99	,118.94	,117.79	,110.09	,106.64	,104.63	,111.12	,111.83	,103.44	,103.82	,103.81	,104.88	,103.36	,84.33	,91.26	,97.95	,85.89	,79.77	,88.34	,83.07	,93.73	,97.5	,94.11	,94.99	,96.52	,101.76	,102.21	,99.09	,100.88	,95.5	,102.48	,103.49]})

rolling_window = 12
ts1_rolling = K_NSI_total["K-NSI"].rolling(window=rolling_window).mean()
K_NSI_total["rolling_K-NSI"] = ts1_rolling
K_NSI_rolling = K_NSI_total.iloc[-12:]

K_NSI_rolling = K_NSI_rolling.reset_index(drop=True)
NSI_m = pd.concat( [NSI_m, K_NSI_rolling], axis=1 )

ts1_diff = K_NSI_total["K-NSI"].diff().dropna()
NSI_m["diff_K-NSI"] = K_NSI_total["K-NSI"] - ts1_diff

K_NSI_rolling = K_NSI_rolling.reset_index( drop=True )





CCSI_total = pd.DataFrame( {"CCSI":[98.5,	100.6,	100.8,	102.7,	98.9	,98.6,	97.0,	93.5,	98.0,	99.7,	102.0,	101.5,	105.3,	98.0,	79.7,	72.1,	78.8,	83.1,	85.4,	89.3,	80.7,	92.7,	99.1,	91.0,	95.3,	97.4,	100.6,	102.4,	105.5,	110.8,	103.6,	102.8,	104.1,	107.3,	108.2,	104.3,	104.9,	103.5,	103.7,	104.3,	103.1,	96.8,	86.2,	89.0,	91.6,	89.0,	86.6,	90.1,	90.7,	90.2,	92.0,	95.2,	98.1,	100.8,	103.4	,103.3,	99.8,	98.2,	97.3,	99.7]})

rolling_window = 12
ts1_rolling = CCSI_total["CCSI"].rolling(window=rolling_window).mean()
CCSI_total["rolling_CCSI"] = ts1_rolling
CCSI_rolling = CCSI_total.iloc[-12:]

CCSI_rolling = CCSI_rolling.reset_index(drop=True)
NSI_m = pd.concat( [NSI_m, CCSI_rolling], axis=1 )

ts1_diff = CCSI_total["CCSI"].diff().dropna()
NSI_m["diff_CCSI"] = CCSI_total["CCSI"] - ts1_diff

# 모든 행을 출력하도록 설정
pd.set_option('display.max_rows', None)

# 모든 열을 출력하도록 설정
pd.set_option('display.max_columns', None)

NSI_m.corr()

### NSI ()
### CCSI (소비자 심리지수) : C1-GPT_P1N (0.621712)
###

from statsmodels.tsa.stattools import ccf

# 교차상관 함수
cross_corr = ccf(NSI_m["GPT_P1N_str_P-NSI_m"], NSI_m["CCSI"])

print( cross_corr )

plt.figure(figsize=(12, 6))
plt.plot(cross_corr)
plt.title('Cross-Correlation Function')
plt.xlabel('Lags')
plt.ylabel('Correlation')
plt.show()



import numpy as np
import matplotlib.pyplot as plt

# 샘플 데이터 생성
np.random.seed(0)
x = NSI_m["CCSI"]
y = NSI_m["GPT_P1N_str_P-NSI_m"]

# 교차 상관관계 계산
lags = np.arange(-len(x) + 1, len(x))
cross_corr = np.correlate(x - np.mean(x), y - np.mean(y), mode='full') / (np.std(x) * np.std(y) * len(x))

# 결과 시각화
plt.figure(figsize=(10, 5))
plt.plot(lags, cross_corr)
plt.title('Cross-Correlation between x and y')
plt.xlabel('Lag')
plt.ylabel('Cross-Correlation Coefficient')
plt.grid()
plt.show()

# 가장 높은 상관 계수와 해당 시간 지연 확인
max_corr_lag = lags[np.argmax(cross_corr)]
max_corr_value = np.max(cross_corr)
print(f"최대 상관 계수: {max_corr_value}, 시간 지연: {max_corr_lag}")



KNSI = pd.read_csv( "/content/drive/MyDrive/Colab Notebooks/Paper/newsdata/NSI_Daily.csv" )
BSI = pd.read_csv( "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/BSI.csv" )
CCSI = pd.read_csv( "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/CCSI_KOR.csv" )
CSI = pd.read_csv( "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/CSI.csv" )
ESI = pd.read_csv( "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/ESI.csv" )

KNSI

CSI











plt.plot( NSI_m["K-NSI"] )
plt.plot( NSI_m["GPT_P1C_P-NSI_m"] )

NSI_m.to_csv( "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/merged_temperature_0.2_GPT_4_C3_NSI.csv", index=False  )

































# from scipy.stats import pearsonr

# correlation, p_value = pearsonr(NSI.iloc[:, -1], NSI.iloc[:,1])

# print( correlation, p_value )

# # Rolling mean 비교
# rolling_window = 5
# ts1_rolling = NSI["K-NSI"].rolling(window=rolling_window).mean()
# ts2_rolling = NSI["GPT_P1_P-NSI"].rolling(window=rolling_window).mean()

# NSI["rolling_K-NSI"] = NSI["K-NSI"] - ts1_rolling
# NSI["rolling_P-KSI"] = NSI["GPT_P1_P-NSI"]

# # plt.figure(figsize=(12, 6))
# # plt.plot(ts1_rolling, label='Time Series 1 Rolling Mean')
# # plt.plot(ts2_rolling, label='Time Series 2 Rolling Mean')
# # plt.plot(ts1_rolling - ts2_rolling, label="distance")
# # plt.title('Rolling Mean Comparison')
# # plt.xlabel('Date')
# # plt.ylabel('Value')
# # plt.legend()
# # plt.show()





# # 차분 계산
# ts1_diff = NSI["K-NSI"].diff().dropna()
# ts2_diff = NSI["GPT_P1_P-NSI"].diff().dropna()

# NSI["diff_K-NSI"] = NSI["K-NSI"] - ts1_diff
# NSI["diff_P-NSI"] = NSI["GPT_P1_P-NSI"]

# # plt.figure(figsize=(12, 6))
# # plt.plot(ts1_diff, label='Time Series 1 Difference')
# # plt.plot(ts2_diff, label='Time Series 2 Difference')
# # plt.title('Time Series Difference Comparison')
# # plt.xlabel('Date')
# # plt.ylabel('Difference')
# # plt.legend()
# # plt.show()

# NSI.corr()



































# plt.figure(figsize=(12, 6))
# plt.plot(NSI["rolling_K-NSI"], label='Time Series 1 Difference')
# plt.plot(NSI["diff_P-NSI"], label='Time Series 2 Difference')
# plt.title('Time Series Difference Comparison')
# plt.xlabel('Date')
# plt.ylabel('Difference')
# plt.legend()
# plt.show()

# ts1_diff - ts2_diff

# from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# # ACF and PACF plot for Time Series 1
# plt.figure(figsize=(12, 6))
# plt.subplot(121)
# plot_acf(NSI["K-NSI"], ax=plt.gca(), lags=50)
# plt.title('ACF of Time Series 1')

# plt.subplot(122)
# plot_pacf(NSI["K-NSI"], ax=plt.gca(), lags=50)
# plt.title('PACF of Time Series 1')

# plt.show()

# # ACF and PACF plot for Time Series 2
# plt.figure(figsize=(12, 6))
# plt.subplot(121)
# plot_acf(NSI["GPT_P1_P-NSI"], ax=plt.gca(), lags=50)
# plt.title('ACF of Time Series 2')

# plt.subplot(122)
# plot_pacf(NSI["GPT_P1_P-NSI"], ax=plt.gca(), lags=50)
# plt.title('PACF of Time Series 2')

# plt.show()













"""### Trend"""

NSI["GPT_P1_trend"] = 0.
NSI["GPT_P2_trend"] = 0.
NSI["GPT_P3_trend"] = 0.
NSI["GPT_P4_trend"] = 0.

NSI["GPT_P1C_trend"] = 0.
NSI["GPT_P2C_trend"] = 0.
NSI["GPT_P3C_trend"] = 0.
NSI["GPT_P4C_trend"] = 0.

NSI["GPT_P1N_trend"] = 0.
NSI["GPT_P2N_trend"] = 0.
NSI["GPT_P3N_trend"] = 0.
NSI["GPT_P4N_trend"] = 0.

NSI["GPT_P1NC_trend"] = 0.
NSI["GPT_P2NC_trend"] = 0.
NSI["GPT_P3NC_trend"] = 0.
NSI["GPT_P4NC_trend"] = 0.

NSI["K_NSI_trend"] = 0.

NSI.columns

for i in range( 0, len(NSI)-1 ):

  NSI.iloc[i, -17] = NSI.iloc[i+1, 1] - NSI.iloc[i, 1]
  NSI.iloc[i, -16] = NSI.iloc[i+1, 2] - NSI.iloc[i, 2]
  NSI.iloc[i, -15] = NSI.iloc[i+1, 3] - NSI.iloc[i, 3]
  NSI.iloc[i, -14] = NSI.iloc[i+1, 4] - NSI.iloc[i, 4]

  NSI.iloc[i, -13] = NSI.iloc[i+1, 5] - NSI.iloc[i, 5]
  NSI.iloc[i, -12] = NSI.iloc[i+1, 6] - NSI.iloc[i, 6]
  NSI.iloc[i, -11] = NSI.iloc[i+1, 7] - NSI.iloc[i, 7]
  NSI.iloc[i, -10] = NSI.iloc[i+1, 8] - NSI.iloc[i, 8]

  NSI.iloc[i, -9] = NSI.iloc[i+1, 9] - NSI.iloc[i, 9]
  NSI.iloc[i, -8] = NSI.iloc[i+1, 10] - NSI.iloc[i, 10]
  NSI.iloc[i, -7] = NSI.iloc[i+1, 11] - NSI.iloc[i, 11]
  NSI.iloc[i, -6] = NSI.iloc[i+1, 12] - NSI.iloc[i, 12]

  NSI.iloc[i, -5] = NSI.iloc[i+1, 13] - NSI.iloc[i, 13]
  NSI.iloc[i, -4] = NSI.iloc[i+1, 14] - NSI.iloc[i, 14]
  NSI.iloc[i, -3] = NSI.iloc[i+1, 15] - NSI.iloc[i, 15]
  NSI.iloc[i, -2] = NSI.iloc[i+1, 16] - NSI.iloc[i, 16]

  NSI.iloc[i, -1] = NSI.iloc[i+1, 17] - NSI.iloc[i, 17]

# NSI[["GPT_P4C_P-NSI", "GPT_P4C_trend"]].head(10)

NSI["GPT_P1_direction"] = 0.
NSI["GPT_P2_direction"] = 0.
NSI["GPT_P3_direction"] = 0.
NSI["GPT_P4_direction"] = 0.

NSI["GPT_P1C_direction"] = 0.
NSI["GPT_P2C_direction"] = 0.
NSI["GPT_P3C_direction"] = 0.
NSI["GPT_P4C_direction"] = 0.

NSI["GPT_P1N_direction"] = 0.
NSI["GPT_P2N_direction"] = 0.
NSI["GPT_P3N_direction"] = 0.
NSI["GPT_P4N_direction"] = 0.

NSI["GPT_P1NC_direction"] = 0.
NSI["GPT_P2NC_direction"] = 0.
NSI["GPT_P3NC_direction"] = 0.
NSI["GPT_P4NC_direction"] = 0.

NSI["K_NSI_direction"] = 0.

for i in range( 0, len(NSI) ):
  sindex = -34
  tindex = -17

  for j in range(0,17):

    if NSI.iloc[i, sindex+j] > 0:
      NSI.iloc[i, tindex+j] = 1
    elif NSI.iloc[i, sindex+j] < 0:
      NSI.iloc[i, tindex+j] = -1
    else:
      NSI.iloc[i, tindex+j] = 0

NSI

NSI["GPT_P1_simularity"] = ""
NSI["GPT_P2_simularity"] = ""
NSI["GPT_P3_simularity"] = ""
NSI["GPT_P4_simularity"] = ""

NSI["GPT_P1C_simularity"] = ""
NSI["GPT_P2C_simularity"] = ""
NSI["GPT_P3C_simularity"] = ""
NSI["GPT_P4C_simularity"] = ""

NSI["GPT_P1N_simularity"] = ""
NSI["GPT_P2N_simularity"] = ""
NSI["GPT_P3N_simularity"] = ""
NSI["GPT_P4N_simularity"] = ""

NSI["GPT_P1NC_simularity"] = ""
NSI["GPT_P2NC_simularity"] = ""
NSI["GPT_P3NC_simularity"] = ""
NSI["GPT_P4NC_simularity"] = ""

NSI["K_NSI_simularity"] = ""

for i in range( 0, len(NSI) ):
  sindex = -34
  tindex = -17

  for j in range(0,17):

    if NSI.iloc[i, sindex+j] == NSI.iloc[i, -18]:
      NSI.iloc[i, tindex+j] = "same"
    else:
      NSI.iloc[i, tindex+j] = "different"

NSI.iloc[:,-17:]

NSI.groupby("GPT_P1_simularity")["GPT_P1_simularity"].count()

NSI.groupby("GPT_P2_simularity")["GPT_P2_simularity"].count()

NSI.groupby("GPT_P3_simularity")["GPT_P3_simularity"].count()

NSI.groupby("GPT_P4_simularity")["GPT_P4_simularity"].count()

NSI.groupby("GPT_P1C_simularity")["GPT_P1C_simularity"].count()

NSI.groupby("GPT_P2C_simularity")["GPT_P2C_simularity"].count()

NSI.groupby("GPT_P3C_simularity")["GPT_P3C_simularity"].count()

NSI.groupby("GPT_P4C_simularity")["GPT_P4C_simularity"].count()

NSI.groupby("GPT_P1N_simularity")["GPT_P1N_simularity"].count()

NSI.groupby("GPT_P1N_simularity")["GPT_P1N_simularity"].count()







"""### Distance"""



NSI["distance"] = NSI["P-NSI"] - NSI["K-NSI"]

NSI["P-NSI_distance"] = 0
NSI["K-NSI_distance"] = 0

for i in range(0, len(NSI)-1):
  NSI.iloc[i, -2] = NSI.iloc[i+1, -5] - NSI.iloc[i, -5]
  NSI.iloc[i, -1] = NSI.iloc[i+1, -3] - NSI.iloc[i, -3]

NSI

# NSI.to_csv( "/content/drive/MyDrive/Colab Notebooks/Paper/newsdata/NSI_240425_1.csv", index=False )

temp = NSI[["date", "P-NSI", "K-NSI", "P-NSI_distance", "K-NSI_distance", "distance"]]

temp2 = temp[["K-NSI", "P-NSI"]]
temp2.corr()

temp2 = temp[["K-NSI_distance", "P-NSI_distance"]]
temp2.corr()

plt.plot( temp["date"], temp["P-NSI"], color="green")
plt.plot( temp["date"], temp["K-NSI"], color="blue")
# plt.plot( temp["date"], temp["distance"], color="red")

plt.show()

plt.plot( temp["date"], temp["P-NSI_distance"], color="red")
plt.plot( temp["date"], temp["K-NSI_distance"], color="gray")