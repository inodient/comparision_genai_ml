# -*- coding: utf-8 -*-
"""paper_result_analysis_FinBERT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PaRb0EGHf9wR8vPMI1YVC9zyr45TZdPh
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set( style="whitegrid" )

from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_curve, roc_auc_score, auc
from sklearn.metrics import confusion_matrix

def preprocession( result_path, marked_path ):

  result = pd.read_csv( result_path )
  marked = pd.read_csv( marked_path )

  result["manual"] = marked["merged_sentiment"]
  result["remove"] = marked["remove"]

  result_subset = result
  result_subset = result_subset[result_subset["remove"] != 1.0]
  result_subset["manual_str"] = result_subset["manual"]

  result_subset= result_subset[result_subset["manual_str"] != "neutral"]
  result_subset= result_subset[result_subset["KR_FinBERT"] != "neutral"]

  # result_subset = result_subset[result_subset["GPT_P1"] != "neutral"]
  # result_subset = result_subset[result_subset["GPT_P2"] != "neutral"]
  # result_subset = result_subset[result_subset["GPT_P3"] != "neutral"]
  # result_subset = result_subset[result_subset["GPT_P4"] != "neutral"]
  # result_subset = result_subset[result_subset["GPT_P1N_str"] != "neutral"]
  # result_subset = result_subset[result_subset["GPT_P2N_str"] != "neutral"]
  # result_subset = result_subset[result_subset["GPT_P3N_str"] != "neutral"]
  # result_subset = result_subset[result_subset["GPT_P4N_str"] != "neutral"]

  # result_subset = result_subset[result_subset["GPT_P1C"] != "neutral"]
  # result_subset = result_subset[result_subset["GPT_P2C"] != "neutral"]
  # result_subset = result_subset[result_subset["GPT_P3C"] != "neutral"]
  # result_subset = result_subset[result_subset["GPT_P4C"] != "neutral"]
  # result_subset = result_subset[result_subset["GPT_P1NC_str"] != "neutral"]
  # result_subset = result_subset[result_subset["GPT_P2NC_str"] != "neutral"]
  # result_subset = result_subset[result_subset["GPT_P3NC_str"] != "neutral"]
  # result_subset = result_subset[result_subset["GPT_P4NC_str"] != "neutral"]

  return result_subset

"""### Using sklearn"""

def plot_sentiment (result_subset):

  df_melted = pd.melt(result_subset, value_vars=['manual_str', 'KR_FinBERT'], var_name='Sentiment_Column', value_name='Sentiment')
  df_melted

  plt.figure(figsize=(6, 4))
  plt.ylabel('Count')
  plt.title('Sentiment Distribution by Category and model')

  ax = sns.countplot(x='Sentiment_Column', hue='Sentiment', data=df_melted)

  for p in ax.patches:
      ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 5), textcoords='offset points')

  plt.show()

def scoring( reputation, engine, ptype, y_true, y_pred ):

  # Calculate precision, recall, accuracy, and F1-score
  precision = precision_score(y_true, y_pred, pos_label="positive")
  recall = recall_score(y_true, y_pred, pos_label="positive")
  accuracy = accuracy_score(y_true, y_pred)
  f1 = f1_score(y_true, y_pred, pos_label="positive")

  y_true_number = y_true.map({'positive': 1, 'negative': 0})
  y_pred_number = y_pred.map({'positive': 1, 'negative': 0})
  fpr, tpr, _ = roc_curve(y_true_number, y_pred_number)
  roc_auc = auc(fpr, tpr)

  plt.figure()
  plt.plot(fpr, tpr, color='blue', lw=2, label='Model B (AUC = %0.2f)' % roc_auc)
  plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
  plt.xlim([0.0, 1.0])
  plt.ylim([0.0, 1.05])
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title('Receiver Operating Characteristic')
  plt.legend(loc="lower right")
  plt.show()

  cnf = confusion_matrix(y_true, y_pred)
  print( cnf )


  # print("Precision:", precision)
  # print("Recall:", recall)
  # print("Accuracy:", accuracy)
  # print("F1-score:", f1)

  return {"reputation": reputation, "engine":engine, "prompt":ptype, "accuracy":accuracy, "precision":precision, "recall":recall, "F1":f1, "AUC":roc_auc}



def calc_result( reputation, engine, result_subset ):
  stat = pd.DataFrame( columns=["reputation", "engine", "prompt", "accuracy", "precision", "recall", "F1", "AUC"] )

  # GPT_P1 = scoring( reputation, engine, "GPT_P1", result_subset["manual_str"], result_subset["GPT_P1"] )
  # GPT_P2 = scoring( reputation, engine, "GPT_P2", result_subset["manual_str"], result_subset["GPT_P2"] )
  # GPT_P3 = scoring( reputation, engine, "GPT_P3", result_subset["manual_str"], result_subset["GPT_P3"] )
  # GPT_P4 = scoring( reputation, engine, "GPT_P4", result_subset["manual_str"], result_subset["GPT_P4"] )
  # GPT_P1N = scoring( reputation, engine, "GPT_P1N", result_subset["manual_str"], result_subset["GPT_P1N_str"] )
  # GPT_P2N = scoring( reputation, engine, "GPT_P2N", result_subset["manual_str"], result_subset["GPT_P2N_str"] )
  # GPT_P3N = scoring( reputation, engine, "GPT_P3N", result_subset["manual_str"], result_subset["GPT_P3N_str"] )
  # GPT_P4N = scoring( reputation, engine, "GPT_P4N", result_subset["manual_str"], result_subset["GPT_P4N_str"] )
  # GPT_P1C = scoring( reputation, engine, "GPT_P1C", result_subset["manual_str"], result_subset["GPT_P1C"] )
  # GPT_P2C = scoring( reputation, engine, "GPT_P2C", result_subset["manual_str"], result_subset["GPT_P2C"] )
  # GPT_P3C = scoring( reputation, engine, "GPT_P3C", result_subset["manual_str"], result_subset["GPT_P3C"] )
  # GPT_P4C = scoring( reputation, engine, "GPT_P4C", result_subset["manual_str"], result_subset["GPT_P4C"] )
  # GPT_P1NC = scoring( reputation, engine, "GPT_P1NC", result_subset["manual_str"], result_subset["GPT_P1NC_str"] )
  # GPT_P2NC = scoring( reputation, engine, "GPT_P2NC", result_subset["manual_str"], result_subset["GPT_P2NC_str"] )
  # GPT_P3NC = scoring( reputation, engine, "GPT_P3NC", result_subset["manual_str"], result_subset["GPT_P3NC_str"] )
  # GPT_P4NC = scoring( reputation, engine, "GPT_P4NC", result_subset["manual_str"], result_subset["GPT_P4NC_str"] )
  KR_FinBERT = scoring( reputation, engine, "KR_FinBERT", result_subset["manual_str"], result_subset["KR_FinBERT"] )

  # stat.loc[len(stat)] = GPT_P1
  # stat.loc[len(stat)] = GPT_P2
  # stat.loc[len(stat)] = GPT_P3
  # stat.loc[len(stat)] = GPT_P4

  # stat.loc[len(stat)] = GPT_P1C
  # stat.loc[len(stat)] = GPT_P2C
  # stat.loc[len(stat)] = GPT_P3C
  # stat.loc[len(stat)] = GPT_P4C

  # stat.loc[len(stat)] = GPT_P1N
  # stat.loc[len(stat)] = GPT_P2N
  # stat.loc[len(stat)] = GPT_P3N
  # stat.loc[len(stat)] = GPT_P4N

  # stat.loc[len(stat)] = GPT_P1NC
  # stat.loc[len(stat)] = GPT_P2NC
  # stat.loc[len(stat)] = GPT_P3NC
  # stat.loc[len(stat)] = GPT_P4NC

  stat.loc[len(stat)] = KR_FinBERT

  return stat

def calc_stat( reputation, engine, result_path, marked_path ):
  result_subset = preprocession( result_path, marked_path )
  plot_sentiment(result_subset)
  stat = calc_result( reputation, engine, result_subset )

  return stat

stat_KR_FinBERT = calc_stat( 1, "KR-FinBERT", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/merged_temperature_0.2_C1.csv", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/news_manual_merged.csv" )
# stat_GPT35_c2 = calc_stat( 2, "GPT-3.5-Turbo", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/merged_temperature_0.2_C2.csv", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/news_manual_merged.csv" )
# stat_GPT35_c3 = calc_stat( 3, "GPT-3.5-Turbo", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/merged_temperature_0.2_C3.csv", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/news_manual_merged.csv" )

# stat_GPT40_c1 = calc_stat( 1, "GPT-4.0-Turbo", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/merged_temperature_0.2_GPT_4_C1.csv", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/news_manual_merged.csv" )
# stat_GPT40_c2 = calc_stat( 2, "GPT-4.0-Turbo", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/merged_temperature_0.2_GPT_4_C2.csv", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/news_manual_merged.csv" )
# stat_GPT40_c3 = calc_stat( 3, "GPT-4.0-Turbo", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/merged_temperature_0.2_GPT_4_C3.csv", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/news_manual_merged.csv" )

stat = stat_KR_FinBERT

stat = stat.reset_index( drop=True )

stat

stat["role"] = "-"

for i in range( len(stat) ):
  if stat.iloc[i, 2] == "GPT_P1" or stat.iloc[i, 2] == "GPT_P1C" or stat.iloc[i, 2] == "GPT_P1N" or stat.iloc[i, 2] == "GPT_P1NC":
    stat.iloc[i, -1] = "P1"
  elif stat.iloc[i, 2] == "GPT_P2" or stat.iloc[i, 2] == "GPT_P2C" or stat.iloc[i, 2] == "GPT_P2N" or stat.iloc[i, 2] == "GPT_P2NC":
    stat.iloc[i, -1] = "P2"
  elif stat.iloc[i, 2] == "GPT_P3" or stat.iloc[i, 2] == "GPT_P3C" or stat.iloc[i, 2] == "GPT_P3N" or stat.iloc[i, 2] == "GPT_P3NC":
    stat.iloc[i, -1] = "P3"
  elif stat.iloc[i, 2] == "GPT_P4" or stat.iloc[i, 2] == "GPT_P4C" or stat.iloc[i, 2] == "GPT_P4N" or stat.iloc[i, 2] == "GPT_P4NC":
    stat.iloc[i, -1] = "P4"

stat["output"] = "-"

for i in range( len(stat) ):
  if stat.iloc[i, 2] == "GPT_P1" or stat.iloc[i, 2] == "GPT_P2" or stat.iloc[i, 2] == "GPT_P3" or stat.iloc[i, 2] == "GPT_P4" or stat.iloc[i, 2] == "GPT_P1C" or stat.iloc[i, 2] == "GPT_P2C" or stat.iloc[i, 2] == "GPT_P3C" or stat.iloc[i, 2] == "GPT_P4C":
    stat.iloc[i, -1] = "binary"
  elif stat.iloc[i, 2] == "GPT_P1N" or stat.iloc[i, 2] == "GPT_P2N" or stat.iloc[i, 2] == "GPT_P3N" or stat.iloc[i, 2] == "GPT_P4N" or stat.iloc[i, 2] == "GPT_P1NC" or stat.iloc[i, 2] == "GPT_P2NC" or stat.iloc[i, 2] == "GPT_P3NC" or stat.iloc[i, 2] == "GPT_P4NC":
    stat.iloc[i, -1] = "numerical"

stat["input"] = "-"

for i in range( len(stat) ):
  if stat.iloc[i, 2] == "GPT_P1" or stat.iloc[i, 2] == "GPT_P2" or stat.iloc[i, 2] == "GPT_P3" or stat.iloc[i, 2] == "GPT_P4" or stat.iloc[i, 2] == "GPT_P1N" or stat.iloc[i, 2] == "GPT_P2N" or stat.iloc[i, 2] == "GPT_P3N" or stat.iloc[i, 2] == "GPT_P4N":
    stat.iloc[i, -1] = "headline"
  elif stat.iloc[i, 2] == "GPT_P1C" or stat.iloc[i, 2] == "GPT_P2C" or stat.iloc[i, 2] == "GPT_P3C" or stat.iloc[i, 2] == "GPT_P4C" or stat.iloc[i, 2] == "GPT_P1NC" or stat.iloc[i, 2] == "GPT_P2NC" or stat.iloc[i, 2] == "GPT_P3NC" or stat.iloc[i, 2] == "GPT_P4NC":
    stat.iloc[i, -1] = "headline + content"

stat.columns

new_order = ['reputation', 'engine', 'role', 'output', 'input', 'prompt', 'accuracy', 'precision', 'recall','F1']
stat = stat.reindex(columns=new_order)



stat.describe()

# plt.figure(figsize=(3, 6))
# sns.boxplot( y="accuracy", data=stat )
# sns.boxplot( y="precision", data=stat )

# # sns.boxplot(x="day", y="total_bill", data=tips)
# # plt.title("Boxplot of Total Bill by Day")
# plt.show()



fig, axes = plt.subplots(1, 4, figsize=(14, 6))

sns.boxplot(y="accuracy", data=stat, ax=axes[0])
# axes[0].set_title("정확도")

sns.boxplot(y="precision", data=stat, ax=axes[1])
# axes[1].set_title("민감도")

sns.boxplot(y="recall", data=stat, ax=axes[2])
# axes[2].set_title("민감도")

sns.boxplot(y="F1", data=stat, ax=axes[3])
# axes[3].set_title("F1-점수")

# 그래프 간격 조정
plt.tight_layout()

# 결과 출력
plt.show()

stat_engine_mean = stat.groupby("engine")[["accuracy", "precision", "recall", "F1"]].mean().reset_index()
stat_engine_std = stat.groupby("engine")[["accuracy", "precision", "recall", "F1"]].std().reset_index()
stat_engine_min = stat.groupby("engine")[["accuracy", "precision", "recall", "F1"]].min().reset_index()
stat_engine_max = stat.groupby("engine")[["accuracy", "precision", "recall", "F1"]].max().reset_index()

stat_engine_mean

stat_engine_std

stat_engine_max

stat_engine_min

stat_engine_range = pd.DataFrame( columns=["engine", "accuracy", "precision", "recall", "F1"])

for i in range(len(stat_engine_max)):
  stat_engine_range.loc[i,"engine"] = stat_engine_max.loc[i,"engine"]
  stat_engine_range.loc[i,"accuracy"] = stat_engine_max.loc[i,"accuracy"] - stat_engine_min.loc[i,"accuracy"]
  stat_engine_range.loc[i,"precision"] = stat_engine_max.loc[i,"precision"] - stat_engine_min.loc[i,"precision"]
  stat_engine_range.loc[i,"recall"] = stat_engine_max.loc[i,"recall"] - stat_engine_min.loc[i,"recall"]
  stat_engine_range.loc[i,"F1"] = stat_engine_max.loc[i,"F1"] - stat_engine_min.loc[i,"F1"]

stat_engine_range["statistic"] = "range"

stat_engine_std["statistic"] = "standard deviation"

recol = ["statistic", "engine", "accuracy", "precision", "recall", "F1"]
stat_engine_range = stat_engine_range.reindex( columns=recol)
stat_engine_std = stat_engine_std.reindex( columns=recol)

stat_engine_diff = pd.concat( [stat_engine_std, stat_engine_range], axis=0 )

stat_engine_diff



stat_p_mean = stat.groupby(["role"])[["accuracy", "precision", "recall", "F1"]].mean().reset_index()

stat_p_std = stat.groupby(["role"])[["accuracy", "precision", "recall", "F1"]].std().reset_index()

stat_p_max = stat.groupby(["role"])[["accuracy", "precision", "recall", "F1"]].max().reset_index()

stat_p_min = stat.groupby(["role"])[["accuracy", "precision", "recall", "F1"]].min().reset_index()

stat_p_range = pd.DataFrame( columns=["role", "accuracy", "precision", "recall", "F1"])

for i in range(len(stat_p_max)):
  stat_p_range.loc[i,"role"] = stat_p_max.loc[i,"role"]
  stat_p_range.loc[i,"accuracy"] = stat_p_max.loc[i,"accuracy"] - stat_p_min.loc[i,"accuracy"]
  stat_p_range.loc[i,"precision"] = stat_p_max.loc[i,"precision"] - stat_p_min.loc[i,"precision"]
  stat_p_range.loc[i,"recall"] = stat_p_max.loc[i,"recall"] - stat_p_min.loc[i,"recall"]
  stat_p_range.loc[i,"F1"] = stat_p_max.loc[i,"F1"] - stat_p_min.loc[i,"F1"]

stat_p_range["statistic"] = "range"
stat_p_mean["statistic"] = "mean"
stat_p_max["statistic"] = "max"
stat_p_min["statistic"] = "min"
stat_p_std["statistic"] = "std"

recol = ["statistic", "role", "accuracy", "precision", "recall", "F1"]
stat_p_range = stat_p_range.reindex( columns=recol)
stat_p_mean = stat_p_mean.reindex( columns=recol)
stat_p_max = stat_p_max.reindex( columns=recol)
stat_p_min = stat_p_min.reindex( columns=recol)
stat_p_std = stat_p_std.reindex( columns=recol)

stat_p_diff = pd.concat( [stat_p_range, stat_p_mean, stat_p_max, stat_p_min, stat_p_std], axis=0 )

stat_p_diff



stat_input_mean = stat.groupby(["input"])[["accuracy", "precision", "recall", "F1"]].mean().reset_index()
stat_input_max  = stat.groupby(["input"])[["accuracy", "precision", "recall", "F1"]].max().reset_index()
stat_input_min  = stat.groupby(["input"])[["accuracy", "precision", "recall", "F1"]].min().reset_index()
stat_input_std  = stat.groupby(["input"])[["accuracy", "precision", "recall", "F1"]].std().reset_index()


stat_input_range = pd.DataFrame( columns=["input", "accuracy", "precision", "recall", "F1"])

for i in range(len(stat_input_max)):
  stat_input_range.loc[i,"input"] = stat_input_max.loc[i,"input"]
  stat_input_range.loc[i,"accuracy"] = stat_input_max.loc[i,"accuracy"] - stat_input_min.loc[i,"accuracy"]
  stat_input_range.loc[i,"precision"] = stat_input_max.loc[i,"precision"] - stat_input_min.loc[i,"precision"]
  stat_input_range.loc[i,"recall"] = stat_input_max.loc[i,"recall"] - stat_input_min.loc[i,"recall"]
  stat_input_range.loc[i,"F1"] = stat_input_max.loc[i,"F1"] - stat_input_min.loc[i,"F1"]

stat_input_range["statistic"] = "range"
stat_input_mean["statistic"] = "mean"
stat_input_max["statistic"] = "max"
stat_input_min["statistic"] = "min"
stat_input_std["statistic"] = "std"

recol = ["statistic", "input", "accuracy", "precision", "recall", "F1"]
stat_input_range = stat_input_range.reindex( columns=recol)
stat_input_mean  = stat_input_mean.reindex( columns=recol)
stat_input_max   = stat_input_max.reindex( columns=recol)
stat_input_min   = stat_input_min.reindex( columns=recol)
stat_input_std   = stat_input_std.reindex( columns=recol)

stat_input_diff = pd.concat( [stat_input_range, stat_input_mean, stat_input_max, stat_input_min, stat_input_std], axis=0 )

stat_input_diff



stat_output_mean = stat.groupby(["output"])[["accuracy", "precision", "recall", "F1"]].mean().reset_index()
stat_output_max  = stat.groupby(["output"])[["accuracy", "precision", "recall", "F1"]].max().reset_index()
stat_output_min  = stat.groupby(["output"])[["accuracy", "precision", "recall", "F1"]].min().reset_index()
stat_output_std  = stat.groupby(["output"])[["accuracy", "precision", "recall", "F1"]].std().reset_index()


stat_output_range = pd.DataFrame( columns=["output", "accuracy", "precision", "recall", "F1"])

for i in range(len(stat_input_max)):
  stat_output_range.loc[i,"output"]     = stat_output_max.loc[i,"output"]
  stat_output_range.loc[i,"accuracy"]  = stat_output_max.loc[i,"accuracy"]  - stat_output_min.loc[i,"accuracy"]
  stat_output_range.loc[i,"precision"] = stat_output_max.loc[i,"precision"] - stat_output_min.loc[i,"precision"]
  stat_output_range.loc[i,"recall"]    = stat_output_max.loc[i,"recall"]    - stat_output_min.loc[i,"recall"]
  stat_output_range.loc[i,"F1"]        = stat_output_max.loc[i,"F1"]        - stat_output_min.loc[i,"F1"]

stat_output_range["statistic"] = "range"
stat_output_mean["statistic"] = "mean"
stat_output_max["statistic"] = "max"
stat_output_min["statistic"] = "min"
stat_output_std["statistic"] = "std"

recol = ["statistic", "output", "accuracy", "precision", "recall", "F1"]
stat_output_range = stat_output_range.reindex( columns=recol)
stat_output_mean  = stat_output_mean.reindex( columns=recol)
stat_output_max   = stat_output_max.reindex( columns=recol)
stat_output_min   = stat_output_min.reindex( columns=recol)
stat_output_std   = stat_output_std.reindex( columns=recol)

stat_output_diff = pd.concat( [stat_output_range, stat_output_mean, stat_output_max, stat_output_min, stat_output_std], axis=0 )

stat_output_diff



stat_reputation_mean = stat.groupby(["reputation"])[["accuracy", "precision", "recall", "F1"]].mean().reset_index()
stat_reputation_max  = stat.groupby(["reputation"])[["accuracy", "precision", "recall", "F1"]].max().reset_index()
stat_reputation_min  = stat.groupby(["reputation"])[["accuracy", "precision", "recall", "F1"]].min().reset_index()
stat_reputation_std  = stat.groupby(["reputation"])[["accuracy", "precision", "recall", "F1"]].std().reset_index()


stat_reputation_range = pd.DataFrame( columns=["reputation", "accuracy", "precision", "recall", "F1"])

for i in range(len(stat_input_max)):
  stat_reputation_range.loc[i,"reputation"]    = stat_reputation_max.loc[i,"reputation"]
  stat_reputation_range.loc[i,"accuracy"]  = stat_reputation_max.loc[i,"accuracy"]  - stat_reputation_min.loc[i,"accuracy"]
  stat_reputation_range.loc[i,"precision"] = stat_reputation_max.loc[i,"precision"] - stat_reputation_min.loc[i,"precision"]
  stat_reputation_range.loc[i,"recall"]    = stat_reputation_max.loc[i,"recall"]    - stat_reputation_min.loc[i,"recall"]
  stat_reputation_range.loc[i,"F1"]        = stat_reputation_max.loc[i,"F1"]        - stat_reputation_min.loc[i,"F1"]

stat_reputation_range["statistic"] = "range"
stat_reputation_mean["statistic"] = "mean"
stat_reputation_max["statistic"] = "max"
stat_reputation_min["statistic"] = "min"
stat_reputation_std["statistic"] = "std"

recol = ["statistic", "reputation", "accuracy", "precision", "recall", "F1"]
stat_reputation_range = stat_reputation_range.reindex( columns=recol)
stat_reputation_mean  = stat_reputation_mean.reindex( columns=recol)
stat_reputation_max   = stat_reputation_max.reindex( columns=recol)
stat_reputation_min   = stat_reputation_min.reindex( columns=recol)
stat_reputation_std   = stat_reputation_std.reindex( columns=recol)

stat_reputation_diff = pd.concat( [stat_reputation_range, stat_reputation_mean, stat_reputation_max, stat_reputation_min, stat_reputation_std], axis=0 )

stat_reputation_diff













stat.head(102)

stat.describe()

stat.groupby(["engine", "reputation"])[["accuracy", "precision", "recall", "F1"]].min().reset_index()

stat.groupby(["engine", "reputation"])[["accuracy", "precision", "recall", "F1"]].max().reset_index()

stat.groupby(["engine", "reputation"])[["accuracy", "precision", "recall", "F1"]].mean().reset_index()

stat.groupby(["engine", "reputation"])[["accuracy", "precision", "recall", "F1"]].std().reset_index()

stat.groupby(["engine"])[["accuracy", "precision", "recall", "F1"]].min().reset_index()

stat.groupby(["engine"])[["accuracy", "precision", "recall", "F1"]].max().reset_index()

stat.groupby(["engine"])[["accuracy", "precision", "recall", "F1"]].mean().reset_index()

stat.groupby(["engine"])[["accuracy", "precision", "recall", "F1"]].std().reset_index()

stat.groupby(["engine", "input" ])[["accuracy", "precision", "recall", "F1"]].min().reset_index()

stat.groupby(["engine", "input" ])[["accuracy", "precision", "recall", "F1"]].max().reset_index()

stat.groupby(["engine", "input" ])[["accuracy", "precision", "recall", "F1"]].mean().reset_index()

stat.groupby(["engine", "input" ])[["accuracy", "precision", "recall", "F1"]].std().reset_index()

stat.groupby(["engine", "output" ])[["accuracy", "precision", "recall", "F1"]].min().reset_index()

stat.groupby(["engine", "output" ])[["accuracy", "precision", "recall", "F1"]].max().reset_index()

stat.groupby(["engine", "output" ])[["accuracy", "precision", "recall", "F1"]].mean().reset_index()

stat.groupby(["engine", "output" ])[["accuracy", "precision", "recall", "F1"]].std().reset_index()

stat.groupby(["engine", "prompt" ])[["accuracy", "precision", "recall", "F1"]].min().reset_index()

stat.groupby(["engine", "prompt" ])[["accuracy", "precision", "recall", "F1"]].max().reset_index()

stat.groupby(["engine", "prompt" ])[["accuracy", "precision", "recall", "F1"]].mean().reset_index()

stat.groupby(["engine", "prompt" ])[["accuracy", "precision", "recall", "F1"]].std().reset_index()

















stat_prompt = stat.groupby("prompt")[["accuracy", "precision", "recall", "F1"]].mean().reset_index()

stat_prompt

stat_reputation = stat.groupby(["engine", "reputation"])[["accuracy", "precision", "recall", "F1"]].mean().reset_index()

stat_reputation

stat_role = stat

stat_role["role"] = "-"

for i in range( len(stat_role) ):
  if stat_role.iloc[i, 2] == "GPT_P1" or stat_role.iloc[i, 2] == "GPT_P1C" or stat_role.iloc[i, 2] == "GPT_P1N" or stat_role.iloc[i, 2] == "GPT_P1NC":
    stat_role.iloc[i, -1] = "P1"
  elif stat_role.iloc[i, 2] == "GPT_P2" or stat_role.iloc[i, 2] == "GPT_P2C" or stat_role.iloc[i, 2] == "GPT_P2N" or stat_role.iloc[i, 2] == "GPT_P2NC":
    stat_role.iloc[i, -1] = "P2"
  elif stat_role.iloc[i, 2] == "GPT_P3" or stat_role.iloc[i, 2] == "GPT_P3C" or stat_role.iloc[i, 2] == "GPT_P3N" or stat_role.iloc[i, 2] == "GPT_P3NC":
    stat_role.iloc[i, -1] = "P3"
  elif stat_role.iloc[i, 2] == "GPT_P4" or stat_role.iloc[i, 2] == "GPT_P4C" or stat_role.iloc[i, 2] == "GPT_P4N" or stat_role.iloc[i, 2] == "GPT_P4NC":
    stat_role.iloc[i, -1] = "P4"

stat_role

stat_role.groupby("role")[["accuracy", "precision", "recall", "F1"]].mean()

stat_role.groupby("role")[["accuracy", "precision", "recall", "F1"]].max()

stat_role.groupby("role")[["accuracy", "precision", "recall", "F1"]].min()

stat_role.groupby("role")[["accuracy", "precision", "recall", "F1"]].max() - stat_role.groupby("role")[["accuracy", "precision", "recall", "F1"]].min()

stat_role.groupby("role")[["accuracy", "precision", "recall", "F1"]].std()

stat_output = stat



stat_output["output"] = "-"

for i in range( len(stat_role) ):
  if stat_output.iloc[i, 2] == "GPT_P1" or stat_output.iloc[i, 2] == "GPT_P2" or stat_output.iloc[i, 2] == "GPT_P3" or stat_output.iloc[i, 2] == "GPT_P4" or stat_output.iloc[i, 2] == "GPT_P1C" or stat_output.iloc[i, 2] == "GPT_P2C" or stat_output.iloc[i, 2] == "GPT_P3C" or stat_output.iloc[i, 2] == "GPT_P4C":
    stat_output.iloc[i, -1] = "binary"
  elif stat_output.iloc[i, 2] == "GPT_P1N" or stat_output.iloc[i, 2] == "GPT_P2N" or stat_output.iloc[i, 2] == "GPT_P3N" or stat_output.iloc[i, 2] == "GPT_P4N" or stat_output.iloc[i, 2] == "GPT_P1NC" or stat_output.iloc[i, 2] == "GPT_P2NC" or stat_output.iloc[i, 2] == "GPT_P3NC" or stat_output.iloc[i, 2] == "GPT_P4NC":
    stat_output.iloc[i, -1] = "numerical"

stat_output.groupby("input")[["accuracy", "precision", "recall", "F1"]].min()

stat_input = stat



stat_input["input"] = "-"

for i in range( len(stat_role) ):
  if stat_input.iloc[i, 2] == "GPT_P1" or stat_input.iloc[i, 2] == "GPT_P2" or stat_input.iloc[i, 2] == "GPT_P3" or stat_input.iloc[i, 2] == "GPT_P4" or stat_input.iloc[i, 2] == "GPT_P1N" or stat_input.iloc[i, 2] == "GPT_P2N" or stat_input.iloc[i, 2] == "GPT_P3N" or stat_input.iloc[i, 2] == "GPT_P4N":
    stat_input.iloc[i, -1] = "headline"
  elif stat_input.iloc[i, 2] == "GPT_P1C" or stat_input.iloc[i, 2] == "GPT_P2C" or stat_input.iloc[i, 2] == "GPT_P3C" or stat_input.iloc[i, 2] == "GPT_P4C" or stat_input.iloc[i, 2] == "GPT_P1NC" or stat_input.iloc[i, 2] == "GPT_P2NC" or stat_input.iloc[i, 2] == "GPT_P3NC" or stat_input.iloc[i, 2] == "GPT_P4NC":
    stat_input.iloc[i, -1] = "headline + content"

stat_input.groupby("output")[["accuracy", "precision", "recall", "F1"]].max() - stat_input.groupby("output")[["accuracy", "precision", "recall", "F1"]].min()

stat_prompt.describe()









stat_GPT40_c2

stat.to_csv( "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/stat.csv", index=False )

stat

stat.loc[(stat["prompt"]=="GPT_P1") & (stat["engine"]=="GPT-3.5-Turbo"), "accuracy"]

from scipy.stats import ttest_ind

def perform_ttest(metric_A, metric_B, metric_name):
    t_stat, p_value = ttest_ind(metric_A, metric_B)

    print( metric_A, metric_B )

    print(f"{metric_name}의 t-검정 결과:")
    print(f"t-통계량: {t_stat}")
    print(f"p-값: {p_value}\n")
    if p_value < 0.05:
        print(f"귀무가설 기각: {metric_name}에서 두 모형의 성능이 통계적으로 유의하게 다릅니다.\n")
    else:
        print(f"귀무가설 채택: {metric_name}에서 두 모형의 성능은 통계적으로 유의한 차이가 없습니다.\n")


t1 = stat.loc[(stat["prompt"]=="GPT_P1"), "accuracy"]
t2 = stat.loc[(stat["prompt"]=="KR_FinBERT"), "accuracy"]



# 각 성능 지표에 대해 t-검정 수행
perform_ttest(t1, t2, "정확도 (Accuracy)")
# perform_ttest(precision_A, precision_B, "정밀도 (Precision)")
# perform_ttest(recall_A, recall_B, "민감도 (Recall)")
# perform_ttest(f1_score_A, f1_score_B, "F1-Score")









"""### FinBERT Comparision

"""

stat_FinBERT = pd.DataFrame( columns=["prompt", "accuracy", "precision", "recall", "F1"] )

GPT_P1 = scoring( "GPT_P1", result_subset["KR_FinBERT"], result_subset["GPT_P1"] )
GPT_P2 = scoring( "GPT_P2", result_subset["KR_FinBERT"], result_subset["GPT_P2"] )
GPT_P3 = scoring( "GPT_P3", result_subset["KR_FinBERT"], result_subset["GPT_P3"] )
GPT_P4 = scoring( "GPT_P4", result_subset["KR_FinBERT"], result_subset["GPT_P4"] )
GPT_P1N = scoring( "GPT_P1N", result_subset["KR_FinBERT"], result_subset["GPT_P1N_str"] )
GPT_P2N = scoring( "GPT_P2N", result_subset["KR_FinBERT"], result_subset["GPT_P2N_str"] )
GPT_P3N = scoring( "GPT_P3N", result_subset["KR_FinBERT"], result_subset["GPT_P3N_str"] )
GPT_P4N = scoring( "GPT_P4N", result_subset["KR_FinBERT"], result_subset["GPT_P4N_str"] )

stat_FinBERT.loc[len(stat_FinBERT)] = GPT_P1
stat_FinBERT.loc[len(stat_FinBERT)] = GPT_P2
stat_FinBERT.loc[len(stat_FinBERT)] = GPT_P3
stat_FinBERT.loc[len(stat_FinBERT)] = GPT_P4
stat_FinBERT.loc[len(stat_FinBERT)] = GPT_P1N
stat_FinBERT.loc[len(stat_FinBERT)] = GPT_P2N
stat_FinBERT.loc[len(stat_FinBERT)] = GPT_P3N
stat_FinBERT.loc[len(stat_FinBERT)] = GPT_P4N

stat_FinBERT

"""### S-MAE"""

S_MAE_P1N = abs(result_subset["positive"] - result_subset["GPT_P1N"]).sum() / len(result_subset)
S_MAE_P2N = abs(result_subset["positive"] - result_subset["GPT_P2N"]).sum() / len(result_subset)
S_MAE_P3N = abs(result_subset["positive"] - result_subset["GPT_P3N"]).sum() / len(result_subset)
S_MAE_P4N = abs(result_subset["positive"] - result_subset["GPT_P4N"]).sum() / len(result_subset)

S_MAE_P1N

S_MAE_P2N

S_MAE_P3N

S_MAE_P4N



from statsmodels.stats.contingency_tables import mcnemar

# 예제 데이터 (혼동 행렬)
#       모델 2 예측
#        Yes   No
# 모델 1  Yes   40   10
# 예측    No    5    45

# 컨팅전시 테이블
table = [[40, 10],
         [5, 45]]

# McNemar 검정 수행
result = mcnemar(table, exact=True)
print('McNemar test p-value:', result.pvalue)

import matplotlib.pyplot as plt
import pandas as pd

# 데이터 생성
data = {
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC'],
    'KR-FinBERT': [0.815625, 0.854839, 0.848849, 0.851833, 0.804624],
    'GPT (Average)': [0.9020572, 0.8983521, 0.9594476, 0.9256280, 0.8837016]
}

df = pd.DataFrame(data)

# plot
fig, ax = plt.subplots(figsize=(12, 8))

bar_width = 0.35
index = range(len(df))

bars1 = plt.bar([i - bar_width/2 for i in index], df['KR-FinBERT'], bar_width, label='KR-FinBERT')
bars2 = plt.bar([i + bar_width/2 for i in index], df['GPT (Average)'], bar_width, label='GPT (Average)')

# 각 Bar 위에 숫자 Label 추가
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.4f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

plt.xlabel('Metric')
plt.ylabel('Scores')
plt.title('Performance Comparison')
plt.xticks(index, df['Metric'])
plt.legend()

plt.tight_layout()
plt.show()