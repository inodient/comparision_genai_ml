# -*- coding: utf-8 -*-
"""paper_result_analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ezO8s8ax8wFEyrDxXMJ65Ex1Rs8ocPml
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set( style="whitegrid" )

from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_curve, roc_auc_score, auc
from sklearn.metrics import confusion_matrix

from statsmodels.stats.contingency_tables import mcnemar

def preprocession( result_path, marked_path ):

  result = pd.read_csv( result_path )
  marked = pd.read_csv( marked_path )

  result["manual"] = marked["merged_sentiment"]
  result["remove"] = marked["remove"]

  result_subset = result
  result_subset = result_subset[result_subset["remove"] != 1.0]
  result_subset["manual_str"] = result_subset["manual"]

  result_subset= result_subset[result_subset["manual_str"] != "neutral"]
  result_subset= result_subset[result_subset["KR_FinBERT"] != "neutral"]

  result_subset = result_subset[result_subset["GPT_P1"] != "neutral"]
  result_subset = result_subset[result_subset["GPT_P2"] != "neutral"]
  result_subset = result_subset[result_subset["GPT_P3"] != "neutral"]
  # result_subset = result_subset[result_subset["GPT_P4"] != "neutral"]
  result_subset = result_subset[result_subset["GPT_P1N_str"] != "neutral"]
  result_subset = result_subset[result_subset["GPT_P2N_str"] != "neutral"]
  result_subset = result_subset[result_subset["GPT_P3N_str"] != "neutral"]
  # result_subset = result_subset[result_subset["GPT_P4N_str"] != "neutral"]

  result_subset = result_subset[result_subset["GPT_P1C"] != "neutral"]
  result_subset = result_subset[result_subset["GPT_P2C"] != "neutral"]
  result_subset = result_subset[result_subset["GPT_P3C"] != "neutral"]
  # result_subset = result_subset[result_subset["GPT_P4C"] != "neutral"]
  result_subset = result_subset[result_subset["GPT_P1NC_str"] != "neutral"]
  result_subset = result_subset[result_subset["GPT_P2NC_str"] != "neutral"]
  result_subset = result_subset[result_subset["GPT_P3NC_str"] != "neutral"]
  # result_subset = result_subset[result_subset["GPT_P4NC_str"] != "neutral"]

  return result_subset

"""### Using sklearn"""

def plot_sentiment (result_subset):

  # df_melted = pd.melt(result_subset, value_vars=['manual_str', 'GPT_P1', 'GPT_P2', 'GPT_P3', 'GPT_P4', 'GPT_P1N_str', 'GPT_P2N_str', 'GPT_P3N_str', 'GPT_P4N_str', 'GPT_P1C', 'GPT_P2C', 'GPT_P3C', 'GPT_P4C', 'GPT_P1NC_str', 'GPT_P2NC_str', 'GPT_P3NC_str', 'GPT_P4NC_str', 'KR_FinBERT'], var_name='Sentiment_Column', value_name='Sentiment')
  df_melted = pd.melt(result_subset, value_vars=['manual_str', 'GPT_P1', 'GPT_P2', 'GPT_P3', 'GPT_P1N_str', 'GPT_P2N_str', 'GPT_P3N_str', 'GPT_P1C', 'GPT_P2C', 'GPT_P3C', 'GPT_P1NC_str', 'GPT_P2NC_str', 'GPT_P3NC_str', 'KR_FinBERT'], var_name='Sentiment_Column', value_name='Sentiment')

  df_melted

  plt.figure(figsize=(25, 9))
  plt.ylabel('Count')
  plt.title('Sentiment Distribution by Category')

  ax = sns.countplot(x='Sentiment_Column', hue='Sentiment', data=df_melted)

  for p in ax.patches:
      ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 5), textcoords='offset points')

  plt.show()

def scoring( reputation, engine, ptype, y_true, y_pred, result_subset ):

  # Calculate precision, recall, accuracy, and F1-score
  precision = precision_score(y_true, y_pred, pos_label="positive")
  recall = recall_score(y_true, y_pred, pos_label="positive")
  accuracy = accuracy_score(y_true, y_pred)
  f1 = f1_score(y_true, y_pred, pos_label="positive")

  y_true_number = y_true.map({'positive': 1, 'negative': 0})
  y_pred_number = y_pred.map({'positive': 1, 'negative': 0})
  fpr, tpr, _ = roc_curve(y_true_number, y_pred_number)
  roc_auc = auc(fpr, tpr)

  label_str = "reputation : " + str(reputation) + "\n" + "engine : " + engine + "\n" + "ptype : " + ptype + "\n" + "(AUC = %0.2f)"

  # plt.figure()
  # plt.plot(fpr, tpr, color='blue', lw=2, label=label_str % roc_auc)
  # plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
  # plt.xlim([0.0, 1.0])
  # plt.ylim([0.0, 1.05])
  # plt.xlabel('False Positive Rate')
  # plt.ylabel('True Positive Rate')
  # plt.title('Receiver Operating Characteristic')
  # plt.legend(loc="lower right")
  # plt.show()

  cnf = confusion_matrix(y_true, y_pred)
  # print( cnf )

  result = mcnemar(cnf, exact=True)
  # print('McNemar test p-value:', result.pvalue)
  manual_tagging_McNemar = result.pvalue

  cnf = confusion_matrix(result_subset["KR_FinBERT"], y_pred )
  # print( cnf )

  result = mcnemar(cnf, exact=True)
  # print('McNemar test p-value:', result.pvalue)
  KR_FinBERT_McNemar = result.pvalue

  MaNemar = {"reputation": reputation, "engine":engine, "prompt":ptype, "manual_tagging_McNemar":manual_tagging_McNemar, "KR_FinBERT_McNemar":KR_FinBERT_McNemar}

  print( MaNemar )

  # print("Precision:", precision)
  # print("Recall:", recall)
  # print("Accuracy:", accuracy)
  # print("F1-score:", f1)

  return {"reputation": reputation, "engine":engine, "prompt":ptype, "accuracy":accuracy, "precision":precision, "recall":recall, "F1":f1, "AUC":roc_auc}

# 예제 데이터 (혼동 행렬)
#       모델 2 예측
#        Yes   No
# 모델 1  Yes   40   10
# 예측    No    5    45

# 컨팅전시 테이블
table = [[40, 10],
         [5, 45]]

# McNemar 검정 수행
result = mcnemar(table, exact=True)
print('McNemar test p-value:', result.pvalue)



def calc_result( reputation, engine, result_subset ):
  stat = pd.DataFrame( columns=["reputation", "engine", "prompt", "accuracy", "precision", "recall", "F1", "AUC"] )

  GPT_P1 = scoring( reputation, engine, "GPT_P1", result_subset["manual_str"], result_subset["GPT_P1"], result_subset )
  GPT_P2 = scoring( reputation, engine, "GPT_P2", result_subset["manual_str"], result_subset["GPT_P2"], result_subset )
  GPT_P3 = scoring( reputation, engine, "GPT_P3", result_subset["manual_str"], result_subset["GPT_P3"], result_subset )
  # GPT_P4 = scoring( reputation, engine, "GPT_P4", result_subset["manual_str"], result_subset["GPT_P4"], result_subset )
  GPT_P1N = scoring( reputation, engine, "GPT_P1N", result_subset["manual_str"], result_subset["GPT_P1N_str"], result_subset )
  GPT_P2N = scoring( reputation, engine, "GPT_P2N", result_subset["manual_str"], result_subset["GPT_P2N_str"], result_subset )
  GPT_P3N = scoring( reputation, engine, "GPT_P3N", result_subset["manual_str"], result_subset["GPT_P3N_str"], result_subset )
  # GPT_P4N = scoring( reputation, engine, "GPT_P4N", result_subset["manual_str"], result_subset["GPT_P4N_str"], result_subset )
  GPT_P1C = scoring( reputation, engine, "GPT_P1C", result_subset["manual_str"], result_subset["GPT_P1C"], result_subset )
  GPT_P2C = scoring( reputation, engine, "GPT_P2C", result_subset["manual_str"], result_subset["GPT_P2C"], result_subset )
  GPT_P3C = scoring( reputation, engine, "GPT_P3C", result_subset["manual_str"], result_subset["GPT_P3C"], result_subset )
  # GPT_P4C = scoring( reputation, engine, "GPT_P4C", result_subset["manual_str"], result_subset["GPT_P4C"], result_subset )
  GPT_P1NC = scoring( reputation, engine, "GPT_P1NC", result_subset["manual_str"], result_subset["GPT_P1NC_str"], result_subset )
  GPT_P2NC = scoring( reputation, engine, "GPT_P2NC", result_subset["manual_str"], result_subset["GPT_P2NC_str"], result_subset )
  GPT_P3NC = scoring( reputation, engine, "GPT_P3NC", result_subset["manual_str"], result_subset["GPT_P3NC_str"], result_subset )
  # GPT_P4NC = scoring( reputation, engine, "GPT_P4NC", result_subset["manual_str"], result_subset["GPT_P4NC_str"], result_subset )
  KR_FinBERT = scoring( reputation, engine, "KR_FinBERT", result_subset["manual_str"], result_subset["KR_FinBERT"], result_subset )

  stat.loc[len(stat)] = GPT_P1
  stat.loc[len(stat)] = GPT_P2
  stat.loc[len(stat)] = GPT_P3
  # stat.loc[len(stat)] = GPT_P4

  stat.loc[len(stat)] = GPT_P1C
  stat.loc[len(stat)] = GPT_P2C
  stat.loc[len(stat)] = GPT_P3C
  # stat.loc[len(stat)] = GPT_P4C

  stat.loc[len(stat)] = GPT_P1N
  stat.loc[len(stat)] = GPT_P2N
  stat.loc[len(stat)] = GPT_P3N
  # stat.loc[len(stat)] = GPT_P4N

  stat.loc[len(stat)] = GPT_P1NC
  stat.loc[len(stat)] = GPT_P2NC
  stat.loc[len(stat)] = GPT_P3NC
  # stat.loc[len(stat)] = GPT_P4NC

  stat.loc[len(stat)] = KR_FinBERT

  return stat

def calc_stat( reputation, engine, result_path, marked_path ):
  result_subset = preprocession( result_path, marked_path )
  plot_sentiment(result_subset)
  stat = calc_result( reputation, engine, result_subset )

  return stat

stat_GPT35_c1 = calc_stat( 1, "GPT-3.5-Turbo", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/merged_temperature_0.2_C1.csv", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/news_manual_merged.csv" )
stat_GPT35_c2 = calc_stat( 2, "GPT-3.5-Turbo", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/merged_temperature_0.2_C2.csv", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/news_manual_merged.csv" )
stat_GPT35_c3 = calc_stat( 3, "GPT-3.5-Turbo", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/merged_temperature_0.2_C3.csv", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/news_manual_merged.csv" )

stat_GPT40_c1 = calc_stat( 1, "GPT-4.0-Turbo", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/merged_temperature_0.2_GPT_4_C1.csv", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/news_manual_merged.csv" )
stat_GPT40_c2 = calc_stat( 2, "GPT-4.0-Turbo", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/merged_temperature_0.2_GPT_4_C2.csv", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/news_manual_merged.csv" )
stat_GPT40_c3 = calc_stat( 3, "GPT-4.0-Turbo", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/merged_temperature_0.2_GPT_4_C4.csv", "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/news_manual_merged.csv" )

stat = pd.DataFrame()

stat = pd.concat( [stat_GPT35_c1, stat_GPT35_c2], axis=0 )
stat = pd.concat( [stat, stat_GPT35_c3], axis=0 )
stat = pd.concat( [stat, stat_GPT40_c1], axis=0 )
stat = pd.concat( [stat, stat_GPT40_c2], axis=0 )
stat = pd.concat( [stat, stat_GPT40_c3], axis=0 )

stat = stat.reset_index( drop=True )





stat["role"] = "-"

for i in range( len(stat) ):
  if stat.iloc[i, 2] == "GPT_P1" or stat.iloc[i, 2] == "GPT_P1C" or stat.iloc[i, 2] == "GPT_P1N" or stat.iloc[i, 2] == "GPT_P1NC":
    stat.iloc[i, -1] = "P1"
  elif stat.iloc[i, 2] == "GPT_P2" or stat.iloc[i, 2] == "GPT_P2C" or stat.iloc[i, 2] == "GPT_P2N" or stat.iloc[i, 2] == "GPT_P2NC":
    stat.iloc[i, -1] = "P2"
  elif stat.iloc[i, 2] == "GPT_P3" or stat.iloc[i, 2] == "GPT_P3C" or stat.iloc[i, 2] == "GPT_P3N" or stat.iloc[i, 2] == "GPT_P3NC":
    stat.iloc[i, -1] = "P3"
  # elif stat.iloc[i, 2] == "GPT_P4" or stat.iloc[i, 2] == "GPT_P4C" or stat.iloc[i, 2] == "GPT_P4N" or stat.iloc[i, 2] == "GPT_P4NC":
  #   stat.iloc[i, -1] = "P4"

stat["output"] = "-"

# for i in range( len(stat) ):
#   if stat.iloc[i, 2] == "GPT_P1" or stat.iloc[i, 2] == "GPT_P2" or stat.iloc[i, 2] == "GPT_P3" or stat.iloc[i, 2] == "GPT_P4" or stat.iloc[i, 2] == "GPT_P1C" or stat.iloc[i, 2] == "GPT_P2C" or stat.iloc[i, 2] == "GPT_P3C" or stat.iloc[i, 2] == "GPT_P4C":
#     stat.iloc[i, -1] = "binary"
#   elif stat.iloc[i, 2] == "GPT_P1N" or stat.iloc[i, 2] == "GPT_P2N" or stat.iloc[i, 2] == "GPT_P3N" or stat.iloc[i, 2] == "GPT_P4N" or stat.iloc[i, 2] == "GPT_P1NC" or stat.iloc[i, 2] == "GPT_P2NC" or stat.iloc[i, 2] == "GPT_P3NC" or stat.iloc[i, 2] == "GPT_P4NC":
#     stat.iloc[i, -1] = "numerical"

for i in range( len(stat) ):
  if stat.iloc[i, 2] == "GPT_P1" or stat.iloc[i, 2] == "GPT_P2" or stat.iloc[i, 2] == "GPT_P3" or stat.iloc[i, 2] == "GPT_P1C" or stat.iloc[i, 2] == "GPT_P2C" or stat.iloc[i, 2] == "GPT_P3C":
    stat.iloc[i, -1] = "binary"
  elif stat.iloc[i, 2] == "GPT_P1N" or stat.iloc[i, 2] == "GPT_P2N" or stat.iloc[i, 2] == "GPT_P3N" or stat.iloc[i, 2] == "GPT_P1NC" or stat.iloc[i, 2] == "GPT_P2NC" or stat.iloc[i, 2] == "GPT_P3NC":
    stat.iloc[i, -1] = "numerical"

stat["input"] = "-"

# for i in range( len(stat) ):
#   if stat.iloc[i, 2] == "GPT_P1" or stat.iloc[i, 2] == "GPT_P2" or stat.iloc[i, 2] == "GPT_P3" or stat.iloc[i, 2] == "GPT_P4" or stat.iloc[i, 2] == "GPT_P1N" or stat.iloc[i, 2] == "GPT_P2N" or stat.iloc[i, 2] == "GPT_P3N" or stat.iloc[i, 2] == "GPT_P4N":
#     stat.iloc[i, -1] = "headline"
#   elif stat.iloc[i, 2] == "GPT_P1C" or stat.iloc[i, 2] == "GPT_P2C" or stat.iloc[i, 2] == "GPT_P3C" or stat.iloc[i, 2] == "GPT_P4C" or stat.iloc[i, 2] == "GPT_P1NC" or stat.iloc[i, 2] == "GPT_P2NC" or stat.iloc[i, 2] == "GPT_P3NC" or stat.iloc[i, 2] == "GPT_P4NC":
#     stat.iloc[i, -1] = "headline + content"

for i in range( len(stat) ):
  if stat.iloc[i, 2] == "GPT_P1" or stat.iloc[i, 2] == "GPT_P2" or stat.iloc[i, 2] == "GPT_P3" or stat.iloc[i, 2] == "GPT_P1N" or stat.iloc[i, 2] == "GPT_P2N" or stat.iloc[i, 2] == "GPT_P3N":
    stat.iloc[i, -1] = "headline"
  elif stat.iloc[i, 2] == "GPT_P1C" or stat.iloc[i, 2] == "GPT_P2C" or stat.iloc[i, 2] == "GPT_P3C" or stat.iloc[i, 2] == "GPT_P1NC" or stat.iloc[i, 2] == "GPT_P2NC" or stat.iloc[i, 2] == "GPT_P3NC":
    stat.iloc[i, -1] = "headline + content"

stat.columns

new_order = ['reputation', 'engine', 'role', 'output', 'input', 'prompt', 'accuracy', 'precision', 'recall','F1', "AUC"]
stat = stat.reindex(columns=new_order)





from scipy.stats import ttest_ind

t_stat, p_value = ttest_ind(stat.loc[stat["prompt"]=="GPT_P3", "accuracy"], stat.loc[stat["prompt"]=="KR_FinBERT", "accuracy"])

# 결과 출력
print("T-statistic:", t_stat)
print("P-value:", p_value)

if p_value < 0.05:
    print("두 모델 간의 성능 차이가 통계적으로 유의미합니다.")
else:
    print("두 모델 간의 성능 차이가 통계적으로 유의미하지 않습니다.")







stat[stat["prompt"]!="KR_FinBERT"].describe()





"""### 5.2.1 : 전체 비교"""

stat_KR_FinBERT = stat.loc[stat["prompt"]=="KR_FinBERT"]
stat_KR_FinBERT.describe()

stat_temp = stat.loc[stat["prompt"]!="KR_FinBERT"]
stat_temp.describe()

import matplotlib.pyplot as plt
import numpy as np

# 데이터
categories = ['accuracy', 'precision', 'recall', 'F1-Score', 'AUC']
kr_finbert_scores = [0.823036, 0.858089, 0.856909, 0.857496, 0.812164]
gpt_scores = [0.895891, 0.891706, 0.959473, 0.921762, 0.875312]

# Bar Chart 설정
x = np.arange(len(categories))  # 카테고리의 위치
width = 0.35  # Bar의 너비

fig, ax = plt.subplots(figsize=(12, 8))
bars1 = ax.bar(x - width/2, kr_finbert_scores, width, label='KR-FinBERT')
bars2 = ax.bar(x + width/2, gpt_scores, width, label='GPT (Average)')

# Label 및 타이틀 설정
ax.set_xlabel('Metrics')
ax.set_ylabel('Scores')
ax.set_title('Comparison of KR-FinBERT and GPT (Average) Scores')
ax.set_xticks(x)
ax.set_xticklabels(categories)
ax.legend()

# Bar 위에 값 표시
def add_labels(bars):
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.3f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

add_labels(bars1)
add_labels(bars2)

# 그래프 출력
plt.tight_layout()
plt.show()

## 5.2.1
stat_temp = stat.loc[stat["prompt"]!="KR_FinBERT"]
stat_temp.groupby( "engine" )[["accuracy", "precision", "recall", "F1", "AUC"]].mean().reset_index()

import matplotlib.pyplot as plt
import numpy as np

# Data
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']
kr_finbert =   [0.823036,0.858089,0.856909,0.857496,0.812164]
gpt_35_turbo = [0.881983,0.870862,0.954482,0.909260,0.859764]
gpt_4_turbo =  [0.909800,0.912550,0.964464,0.934265,0.890860]

# Setting the positions and width for the bars
positions = np.arange(len(metrics))
width = 0.2

# Plotting the bars
fig, ax = plt.subplots(figsize=(12, 10))  # 높이를 더 크게 설정
bar1 = ax.bar(positions - width, kr_finbert, width, label='KR-FinBERT')
bar2 = ax.bar(positions, gpt_35_turbo, width, label='GPT-3.5-Turbo')
bar3 = ax.bar(positions + width, gpt_4_turbo, width, label='GPT-4-Turbo')

# Adding labels
ax.set_xlabel('Metrics')
ax.set_ylabel('Scores')
ax.set_title('Comparison of KR-FinBERT, GPT-3.5-Turbo, and GPT-4-Turbo on Various Metrics')
ax.set_xticks(positions)
ax.set_xticklabels(metrics)

# 범례를 차트 안에 유지하되, 적절한 위치로 이동
ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3)

# Adding numeric labels on top of each bar
def add_labels(bars):
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.4f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

add_labels(bar1)
add_labels(bar2)
add_labels(bar3)

# Adjust layout to make space for the legend
plt.tight_layout(rect=[0, 0.05, 1, 1])  # 하단 여백을 늘려 범례가 차트 밖으로 벗어나지 않게 설정

# Display the plot
plt.show()

## 5.2.1
stat_temp = stat.loc[stat["prompt"]!="KR_FinBERT"]
stat_temp.groupby( "role" )[["accuracy", "precision", "recall", "F1", "AUC"]].mean().reset_index()

p1 = [0.896722, 0.90137, 0.949207, 0.921708, 0.87912]

p1 = np.round( p1, 2)

p1

import matplotlib.pyplot as plt
import numpy as np

# Data
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']
kr_finbert =   [0.823036,0.858089,0.856909,0.857496,0.812164]
p1 = [0.891646,0.895933,0.948167,0.918174,0.872859]
p2 = [0.905512,0.895891,0.966136,0.928081,0.886278]
p3 = [0.890516,0.883295,0.964116,0.919032,0.866799]

kr_finbert = np.round( kr_finbert, 2 )
p1 = np.round( p1, 2 )
p2 = np.round( p2, 2 )
p3 = np.round( p3, 2 )

# Setting the positions and width for the bars
positions = np.arange(len(metrics))
width = 0.13

# Plotting the bars
fig, ax = plt.subplots(figsize=(12, 10))
bar0 = ax.bar(positions - 2*width, kr_finbert, width, label='KR-FinBERT')
bar1 = ax.bar(positions - width, p1, width, label='P1')
bar2 = ax.bar(positions, p2, width, label='P2')
bar3 = ax.bar(positions + width, p3, width, label='P3')

# Adding labels
ax.set_xlabel('Metrics')
ax.set_ylabel('Scores')
ax.set_title('Comparison of KR-FinBERT, P1, P2, and P3 on Various Metrics')
ax.set_xticks(positions)
ax.set_xticklabels(metrics)

# 범례를 차트 안에 유지하되, 한 줄로 표시
ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=4)

# Adding numeric labels on top of each bar, rounded to three decimal places
def add_labels(bars):
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.3f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

add_labels(bar0)
add_labels(bar1)
add_labels(bar2)
add_labels(bar3)

# Adjust layout to prevent label overlap
plt.tight_layout()

# Display the plot
plt.show()

## 5.2.1
stat_temp = stat.loc[stat["prompt"]!="KR_FinBERT"]
stat_temp.groupby( "input" )[["accuracy", "precision", "recall", "F1", "AUC"]].mean().reset_index()

import matplotlib.pyplot as plt
import numpy as np

# Data
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']
kr_finbert =   [0.823036,0.858089,0.856909,0.857496,0.812164]
headline = [0.901998, 0.898684, 0.960715, 0.926215, 0.883136]
headline_content = [0.899298,0.895371,0.957186,0.923069,0.880671]

# Setting the positions and width for the bars
positions = np.arange(len(metrics))
width = 0.2

# Plotting the bars
fig, ax = plt.subplots(figsize=(12, 10))
bar0 = ax.bar(positions - width, kr_finbert, width, label='KR-FinBERT')
bar1 = ax.bar(positions, headline, width, label='headline')
bar2 = ax.bar(positions + width, headline_content, width, label='headline + content')

# Adding labels
ax.set_xlabel('Metrics')
ax.set_ylabel('Scores')
ax.set_title('Comparison of KR-FinBERT, headline, and headline + content on Various Metrics')
ax.set_xticks(positions)
ax.set_xticklabels(metrics)
ax.legend()

ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=4)


# Adding numeric labels on top of each bar, rounded to three decimal places
def add_labels(bars):
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.3f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

add_labels(bar0)
add_labels(bar1)
add_labels(bar2)

# Adjust layout to prevent label overlap
plt.tight_layout()

# Display the plot
plt.show()

## 5.2.1
stat_temp = stat.loc[stat["prompt"]!="KR_FinBERT"]
stat_temp.groupby( "output" )[["accuracy", "precision", "recall", "F1", "AUC"]].mean().reset_index()

import matplotlib.pyplot as plt
import numpy as np

# Data
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']
kr_finbert = [0.815625, 0.854839, 0.848849, 0.851833, 0.804624]
binary = [0.926128,	0.927470,	0.958827,	0.941993,	0.916144]
numerical = [0.865655,	0.855942,	0.960119,	0.901531,	0.834480]

# Setting the positions and width for the bars
positions = np.arange(len(metrics))
width = 0.2

# Plotting the bars
fig, ax = plt.subplots(figsize=(12, 10))
bar0 = ax.bar(positions - width, kr_finbert, width, label='KR-FinBERT')
bar1 = ax.bar(positions, binary, width, label='binary')
bar2 = ax.bar(positions + width, numerical, width, label='numerical')

# Adding labels
ax.set_xlabel('Metrics')
ax.set_ylabel('Scores')
ax.set_title('Comparison of KR-FinBERT, binary, and numerical on Various Metrics')
ax.set_xticks(positions)
ax.set_xticklabels(metrics)
ax.legend()

ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=4)

# Adding numeric labels on top of each bar, rounded to three decimal places
def add_labels(bars):
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.3f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

add_labels(bar0)
add_labels(bar1)
add_labels(bar2)

# Adjust layout to prevent label overlap
plt.tight_layout()

# Display the plot
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Data
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']
kr_finbert = [0.815625, 0.854839, 0.848849, 0.851833, 0.804624]
gpt_average = [0.9020572, 0.8983521, 0.9594476, 0.925628, 0.8837016]

# Setting the positions and width for the bars
positions = np.arange(len(metrics))
width = 0.2

# Plotting the bars
fig, ax = plt.subplots(figsize=(12, 7))
bar0 = ax.bar(positions - width/2, kr_finbert, width, label='KR-FinBERT')
bar1 = ax.bar(positions + width/2, gpt_average, width, label='GPT (Average)')

# Adding labels
ax.set_xlabel('Metrics')
ax.set_ylabel('Scores')
ax.set_title('Comparison of KR-FinBERT and GPT (Average) on Various Metrics')
ax.set_xticks(positions)
ax.set_xticklabels(metrics)
ax.legend()

# Adding numeric labels on top of each bar, rounded to three decimal places
def add_labels(bars):
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.3f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

add_labels(bar0)
add_labels(bar1)

# Adjust layout to prevent label overlap
plt.tight_layout()

# Display the plot
plt.show()







## 5.2.2
stat_temp = stat.loc[ (stat["prompt"]!="KR_FinBERT") ]
stat_temp.describe()

## 5.2.2
stat_temp = stat.loc[ (stat["prompt"]!="KR_FinBERT") ]
stat_temp.groupby( "reputation" )[["accuracy", "precision", "recall", "F1", "AUC"]].mean().reset_index()

## 5.2.2
stat_temp = stat.loc[ (stat["prompt"]!="KR_FinBERT") ]
stat_temp.groupby( "reputation" )[["accuracy", "precision", "recall", "F1", "AUC"]].mean().reset_index()



import pandas as pd
import matplotlib.pyplot as plt

# Data
data = {
    'reputation': [1, 2, 3],
    'accuracy': [0.917592, 0.917247, 0.871333],
    'precision': [0.914344, 0.910229, 0.870484],
    'recall': [0.959279, 0.964786, 0.954278],
    'F1': [0.935577, 0.935377, 0.905931],
    'AUC': [0.90445, 0.903702, 0.842954]
}

# Create DataFrame
df = pd.DataFrame(data)

# Round data to three decimal places
df = df.round(3)

# Plot
plt.figure(figsize=(10, 6))

for column in df.columns[1:]:
    plt.plot(df['reputation'], df[column], marker='o', label=column)
    # Annotate each point with the value
    for i in range(len(df)):
        plt.text(df['reputation'][i], df[column][i], str(df[column][i]), fontsize=9, ha='right')

plt.xlabel('Reputation')
plt.ylabel('Values')
plt.title('Line Chart with Reputation vs Accuracy, Precision, Recall, F1, AUC')
plt.legend()
plt.grid(True)
plt.xticks(df['reputation'])  # Ensuring the x-axis is clearly labeled with reputation values
plt.show()

## 5.2.2
stat_temp = stat.loc[ (stat["prompt"]!="KR_FinBERT") & (stat["engine"]=="GPT-3.5-Turbo") ]
stat_temp.groupby( "prompt" )[["accuracy", "precision", "recall", "F1", "AUC"]].mean().reset_index()

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'stat' is the DataFrame you're working with
# Ensure you have imported pandas as pd if needed

fig, axes = plt.subplots(1, 5, figsize=(14, 6), sharey=True)  # Add sharey=True to align y-axes

# Define the y-axis limit for consistent scaling
y_lim = (stat[["accuracy", "precision", "recall", "F1", "AUC"]].min().min(),
         stat[["accuracy", "precision", "recall", "F1", "AUC"]].max().max())

sns.boxplot(y="accuracy", data=stat[stat["prompt"] != "KR_FinBERT"], ax=axes[0])
axes[0].set_ylim(y_lim)  # Set y-axis limits
axes[0].set_title("Accuracy")

sns.boxplot(y="precision", data=stat[stat["prompt"] != "KR_FinBERT"], ax=axes[1])
axes[1].set_ylim(y_lim)  # Set y-axis limits
axes[1].set_title("Precision")

sns.boxplot(y="recall", data=stat[stat["prompt"] != "KR_FinBERT"], ax=axes[2])
axes[2].set_ylim(y_lim)  # Set y-axis limits
axes[2].set_title("Recall")

sns.boxplot(y="F1", data=stat[stat["prompt"] != "KR_FinBERT"], ax=axes[3])
axes[3].set_ylim(y_lim)  # Set y-axis limits
axes[3].set_title("F1 Score")

sns.boxplot(y="AUC", data=stat[stat["prompt"] != "KR_FinBERT"], ax=axes[4])
axes[4].set_ylim(y_lim)  # Set y-axis limits
axes[4].set_title("AUC")

# Adjust layout
plt.tight_layout()

# Display the plot
plt.show()



stat_temp = stat.loc[(stat["prompt"]!="KR_FinBERT") & (stat["engine"]=="GPT-3.5-Turbo")]
stat_temp.describe()

stat_temp = stat.loc[(stat["prompt"]!="KR_FinBERT") & (stat["engine"]=="GPT-4.0-Turbo")]
stat_temp.describe()

import seaborn as sns
import matplotlib.pyplot as plt

# 데이터 필터링
data_3_5 = stat.loc[(stat["prompt"] != "KR_FinBERT") & (stat["engine"] == "GPT-3.5-Turbo")]
data_4_0 = stat.loc[(stat["prompt"] != "KR_FinBERT") & (stat["engine"] == "GPT-4.0-Turbo")]

# 공통된 y축 범위 설정
metrics = ["accuracy", "precision", "recall", "F1", "AUC"]
y_min = min(data_3_5[metrics].min().min(), data_4_0[metrics].min().min())
y_max = max(data_3_5[metrics].max().max(), data_4_0[metrics].max().max())

# Plotting function
def plot_metric(metric):
    fig, axes = plt.subplots(1, 2, figsize=(7, 5))
    sns.boxplot(y=metric, data=data_3_5, ax=axes[0])
    sns.boxplot(y=metric, data=data_4_0, ax=axes[1])

    # y축 범위 설정
    axes[0].set_ylim(y_min, y_max)
    axes[1].set_ylim(y_min, y_max)

    # 제목 설정
    axes[0].set_title(f"GPT-3.5-Turbo - {metric.capitalize()}")
    axes[1].set_title(f"GPT-4-Turbo - {metric.capitalize()}")

    plt.tight_layout()
    plt.show()

# Plot all metrics
for metric in metrics:
    plot_metric(metric)



stat_temp = stat.loc[(stat["prompt"]!="KR_FinBERT") & (stat["engine"]=="GPT-3.5-Turbo")]
stat_prompt_group = stat_temp.groupby("prompt")[["accuracy", "precision", "recall", "F1", "AUC"]].mean().reset_index()

stat_prompt_group

import pandas as pd
from sklearn.preprocessing import StandardScaler

# StandardScaler를 이용해 표준화
scaler = StandardScaler()
stat_prompt_group[['accuracy', 'precision', 'recall', 'F1', 'AUC']] = scaler.fit_transform(stat_prompt_group[['accuracy', 'precision', 'recall', 'F1', 'AUC']])

# import ace_tools as tools; tools.display_dataframe_to_user(name="Standardized Data", dataframe=df)

print(stat_prompt_group)

import pandas as pd
from sklearn.preprocessing import StandardScaler
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap

# 히트맵을 그리기 위해 데이터 프레임을 피벗
df_pivot = stat_prompt_group.set_index('prompt')

# 커스텀 컬러 맵 생성 (검은색이 섞인 붉은색 계통)
colors = [(0, 0, 0), (0.5, 0, 0), (1, 0, 0)]  # 검은색, 어두운 붉은색, 밝은 붉은색
n_bins = 100  # 컬러 맵의 세분화 정도
cmap_name = 'black_red'
cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)

# 히트맵 그리기
plt.figure(figsize=(10, 7))
sns.heatmap(df_pivot, annot=True, cmap=cm, center=0)
plt.title('Standardized Metrics Heatmap - GPT-3.5-Turbo')
plt.show()

stat_temp = stat.loc[(stat["prompt"]!="KR_FinBERT") & (stat["engine"]=="GPT-4.0-Turbo")]
stat_prompt_group = stat_temp.groupby("prompt")[["accuracy", "precision", "recall", "F1", "AUC"]].mean().reset_index()

stat_prompt_group

import pandas as pd
from sklearn.preprocessing import StandardScaler

# StandardScaler를 이용해 표준화
scaler = StandardScaler()
stat_prompt_group[['accuracy', 'precision', 'recall', 'F1', 'AUC']] = scaler.fit_transform(stat_prompt_group[['accuracy', 'precision', 'recall', 'F1', 'AUC']])

# import ace_tools as tools; tools.display_dataframe_to_user(name="Standardized Data", dataframe=df)

print(stat_prompt_group)

import pandas as pd
from sklearn.preprocessing import StandardScaler
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap

# import ace_tools as tools; tools.display_dataframe_to_user(name="Standardized Metrics", dataframe=df)

# 히트맵을 그리기 위해 데이터 프레임을 피벗
df_pivot = stat_prompt_group.set_index('prompt')

# 커스텀 컬러 맵 생성 (검은색이 섞인 붉은색 계통)
colors = [(0, 0, 0), (0.5, 0, 0), (1, 0, 0)]  # 검은색, 어두운 붉은색, 밝은 붉은색
n_bins = 100  # 컬러 맵의 세분화 정도
cmap_name = 'black_red'
cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)

# 히트맵 그리기
plt.figure(figsize=(12, 8))
sns.heatmap(df_pivot, annot=True, cmap=cm, center=0)
plt.title('Standardized Metrics Heatmap - GPT-4-Turbo')
plt.show()

stat_temp = stat.loc[ (stat["prompt"]!="KR_FinBERT")]
# stat_temp.groupby( "engine" )[["accuracy", "precision", "recall", "F1", "AUC"]].mean().reset_index()

stat_temp_35 = stat_temp.loc[stat_temp["engine"]=="GPT-3.5-Turbo", ["accuracy", "precision", "recall", "F1", "AUC"]]

stat_temp_40 = stat_temp.loc[stat_temp["engine"]=="GPT-4.0-Turbo", ["accuracy", "precision", "recall", "F1", "AUC"]]

stat_temp_35

import pandas as pd

# GPT-3.5-Turbo 데이터
data_3_5 = {
    'stat': ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'],
    'accuracy': [48.000000, 0.893646, 0.029356, 0.775862, 0.883034, 0.901244, 0.912923, 0.932049],
    'precision': [48.000000, 0.883137, 0.039937, 0.729529, 0.861465, 0.887638, 0.914409, 0.936594],
    'recall': [48.000000, 0.955842, 0.034888, 0.832215, 0.949385, 0.969112, 0.977610, 0.994924],
    'F1': [48.000000, 0.916888, 0.020984, 0.841804, 0.908090, 0.922383, 0.930907, 0.943933],
    'AUC': [48.000000, 0.875727, 0.037777, 0.721513, 0.858093, 0.886093, 0.895677, 0.926524]
}

# GPT-4-Turbo 데이터
data_4 = {
    'stat': ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'],
    'accuracy': [48.000000, 0.905153, 0.085372, 0.633721, 0.916344, 0.939568, 0.949523, 0.960712],
    'precision': [48.000000, 0.908805, 0.096499, 0.633721, 0.918975, 0.945404, 0.969316, 0.983432],
    'recall': [48.000000, 0.963592, 0.027187, 0.847095, 0.959906, 0.967097, 0.973172, 1.000000],
    'F1': [48.000000, 0.931457, 0.051243, 0.775801, 0.932154, 0.951993, 0.959872, 0.968842],
    'AUC': [48.000000, 0.884266, 0.121380, 0.500000, 0.910550, 0.933365, 0.949523, 0.959424]
}

# 데이터 프레임 생성
df_3_5 = pd.DataFrame(data_3_5)
df_4 = pd.DataFrame(data_4)

# 데이터 병합
df_merged = pd.merge(df_3_5, df_4, on='stat', suffixes=('_3_5', '_4'))

# 결과 출력
# import ace_tools as tools; tools.display_dataframe_to_user(name="Merged Metrics", dataframe=df_merged)

print(df_merged)







stat_temp = stat.loc[stat["prompt"]!="KR_FinBERT"]
stat_temp_35

stat_temp_35 = stat_temp.loc[stat_temp["engine"]=="GPT-3.5-Turbo"]
stat_temp_35 = stat_temp_35.groupby(["prompt", "role", "input", "output"])[["accuracy", "precision", "recall", "F1", "AUC"]].agg(["mean", "max", "min", "std"]).reset_index()
stat_temp_35.to_csv( "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/prompt_comparision.csv", index=False)



stat_temp_40 = stat_temp.loc[stat_temp["engine"]=="GPT-4.0-Turbo"]
stat_temp_40 = stat_temp_40.groupby(["prompt", "role", "input", "output"])[["accuracy", "precision", "recall", "F1", "AUC"]].agg(["mean", "max", "min", "std"]).reset_index()
stat_temp_40.to_csv( "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/prompt_comparision_gpt40.csv", index=False)

stat_temp_35 = stat_temp.loc[stat_temp["engine"]=="GPT-3.5-Turbo"]
stat_temp_35 = stat_temp_35.groupby(["prompt", "role", "input", "output"])[["accuracy", "precision", "recall", "F1", "AUC"]].mean().reset_index()

stat_temp_40 = stat_temp.loc[stat_temp["engine"]=="GPT-4.0-Turbo"]
stat_temp_40 = stat_temp_40.groupby(["prompt", "role", "input", "output"])[["accuracy", "precision", "recall", "F1", "AUC"]].mean().reset_index()

stat_temp_35.sort_values(by='accuracy', ascending=False).reset_index(drop=True)

stat_temp_40.sort_values(by='accuracy', ascending=False).reset_index(drop=True)

stat_gpt30 = stat.loc[stat["engine"]=="GPT-3.5-Turbo"]
stat_gpt30.sort_values( by="accuracy", ascending=False ).reset_index(drop=True)

stat_gpt40 = stat.loc[stat["engine"]=="GPT-4.0-Turbo"]
stat_gpt40.sort_values( by="accuracy", ascending=False ).reset_index(drop=True)

import pandas as pd
import matplotlib.pyplot as plt

# 데이터 준비
data = {
    'Category': ['P1', 'P2', 'P3', 'P4', 'binary', 'numerical', 'headline', 'headline+content'],
    'Count_1': [3, 2, 2, 3, 7, 3, 7, 3],
    'Count_2': [4, 2, 2, 2, 10, 0, 7, 3]
}

df = pd.DataFrame(data)

# 시각화
fig, ax = plt.subplots(figsize=(10, 6))

bar_width = 0.35
index = range(len(df['Category']))

bar1 = plt.bar(index, df['Count_1'], bar_width, label='Dataset 1')
bar2 = plt.bar([i + bar_width for i in index], df['Count_2'], bar_width, label='Dataset 2')

plt.xlabel('Category')
plt.ylabel('Count')
plt.title('Category Counts for Two Datasets')
plt.xticks([i + bar_width / 2 for i in index], df['Category'])
plt.legend()

# 그래프 표시
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# 데이터 준비
labels = ['P1', 'P2', 'P3']
sizes = [4,4,2]

# 파이 차트 그리기
fig, ax = plt.subplots()
ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)

# 균형 잡힌 파이 차트를 위해 축을 'equal'로 설정
ax.axis('equal')

# 제목 설정
plt.title('Distribution of role Categories - GPT-3.5-Turbo')

# 그래프 표시
plt.show()

import matplotlib.pyplot as plt

# 데이터 준비
labels = ['P1', 'P2', 'P3']
sizes = [4,3,3]

# 파이 차트 그리기
fig, ax = plt.subplots()
ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)

# 균형 잡힌 파이 차트를 위해 축을 'equal'로 설정
ax.axis('equal')

# 제목 설정
plt.title('Distribution of role Categories - GPT-4-Turbo')

# 그래프 표시
plt.show()

import matplotlib.pyplot as plt

# 데이터 준비
categories = ['binary', 'numerical']
counts = [14, 6]

# 총 합계 계산
total = sum(counts)

# 파스텔 색상 설정
colors = ['#AEC6CF', '#FFB347']  # 파스텔 계통의 색상

# 막대 차트 그리기
fig, ax = plt.subplots()

bars = ax.bar(categories, counts, color=colors)

# 각 막대 위에 백분율 표시
for bar, count in zip(bars, counts):
    percentage = f'{(count / total) * 100:.1f}%'
    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() - 0.1, percentage, ha='center', va='bottom', color='black', fontsize=12)

# 축과 제목 설정
ax.set_xlabel('Category')
ax.set_ylabel('Count')
ax.set_title('Count of Input Type Categories - GPT-3.5-Turbo and GPT-4-Turbo')

# 그래프 표시
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# 데이터 준비
categories = ['headline', 'headline+content']
counts = [19, 1]

# 총 합계 계산
total = sum(counts)

# Seaborn 스타일 설정 및 채도가 있는 색상 팔레트 설정
sns.set(style="whitegrid")
colors = ['#AEC6CF', '#FFB347']  # 파스텔 계통의 색상

# 막대 차트 그리기
fig, ax = plt.subplots()

bars = ax.bar(categories, counts, color=colors)

# 각 막대 위에 백분율 표시
for bar, count in zip(bars, counts):
    percentage = f'{(count / total) * 100:.1f}%'
    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), percentage, ha='center', va='bottom', color='black', fontsize=12)

# 축과 제목 설정
ax.set_xlabel('Category')
ax.set_ylabel('Count')
ax.set_title('Count of Output Type Categories - GPT-3.5-Turbo and GPT-4-Turbo')

# 그래프 표시
plt.show()















# plt.figure(figsize=(3, 6))
# sns.boxplot( y="accuracy", data=stat )
# sns.boxplot( y="precision", data=stat )

# # sns.boxplot(x="day", y="total_bill", data=tips)
# # plt.title("Boxplot of Total Bill by Day")
# plt.show()



fig, axes = plt.subplots(1, 5, figsize=(14, 6))

sns.boxplot(y="accuracy", data=stat[stat["prompt"]!="KR_FinBERT"], ax=axes[0])
# axes[0].set_title("정확도")

sns.boxplot(y="precision", data=stat[stat["prompt"]!="KR_FinBERT"], ax=axes[1])
# axes[1].set_title("민감도")

sns.boxplot(y="recall", data=stat[stat["prompt"]!="KR_FinBERT"], ax=axes[2])
# axes[2].set_title("민감도")

sns.boxplot(y="F1", data=stat[stat["prompt"]!="KR_FinBERT"], ax=axes[3])

sns.boxplot(y="AUC", data=stat[stat["prompt"]!="KR_FinBERT"], ax=axes[4])
# axes[3].set_title("F1-점수")

# 그래프 간격 조정
plt.tight_layout()

# 결과 출력
plt.show()

stat_engine_mean = stat.groupby("engine")[["accuracy", "precision", "recall", "F1"]].mean().reset_index()
stat_engine_std = stat.groupby("engine")[["accuracy", "precision", "recall", "F1"]].std().reset_index()
stat_engine_min = stat.groupby("engine")[["accuracy", "precision", "recall", "F1"]].min().reset_index()
stat_engine_max = stat.groupby("engine")[["accuracy", "precision", "recall", "F1"]].max().reset_index()

stat_engine_mean

stat_engine_std

stat_engine_max

stat_engine_min

stat_engine_range = pd.DataFrame( columns=["engine", "accuracy", "precision", "recall", "F1"])

for i in range(len(stat_engine_max)):
  stat_engine_range.loc[i,"engine"] = stat_engine_max.loc[i,"engine"]
  stat_engine_range.loc[i,"accuracy"] = stat_engine_max.loc[i,"accuracy"] - stat_engine_min.loc[i,"accuracy"]
  stat_engine_range.loc[i,"precision"] = stat_engine_max.loc[i,"precision"] - stat_engine_min.loc[i,"precision"]
  stat_engine_range.loc[i,"recall"] = stat_engine_max.loc[i,"recall"] - stat_engine_min.loc[i,"recall"]
  stat_engine_range.loc[i,"F1"] = stat_engine_max.loc[i,"F1"] - stat_engine_min.loc[i,"F1"]

stat_engine_range["statistic"] = "range"

stat_engine_std["statistic"] = "standard deviation"

recol = ["statistic", "engine", "accuracy", "precision", "recall", "F1"]
stat_engine_range = stat_engine_range.reindex( columns=recol)
stat_engine_std = stat_engine_std.reindex( columns=recol)

stat_engine_diff = pd.concat( [stat_engine_std, stat_engine_range], axis=0 )

stat_engine_diff



stat_p_mean = stat.groupby(["role"])[["accuracy", "precision", "recall", "F1"]].mean().reset_index()

stat_p_std = stat.groupby(["role"])[["accuracy", "precision", "recall", "F1"]].std().reset_index()

stat_p_max = stat.groupby(["role"])[["accuracy", "precision", "recall", "F1"]].max().reset_index()

stat_p_min = stat.groupby(["role"])[["accuracy", "precision", "recall", "F1"]].min().reset_index()

stat_p_range = pd.DataFrame( columns=["role", "accuracy", "precision", "recall", "F1"])

for i in range(len(stat_p_max)):
  stat_p_range.loc[i,"role"] = stat_p_max.loc[i,"role"]
  stat_p_range.loc[i,"accuracy"] = stat_p_max.loc[i,"accuracy"] - stat_p_min.loc[i,"accuracy"]
  stat_p_range.loc[i,"precision"] = stat_p_max.loc[i,"precision"] - stat_p_min.loc[i,"precision"]
  stat_p_range.loc[i,"recall"] = stat_p_max.loc[i,"recall"] - stat_p_min.loc[i,"recall"]
  stat_p_range.loc[i,"F1"] = stat_p_max.loc[i,"F1"] - stat_p_min.loc[i,"F1"]

stat_p_range["statistic"] = "range"
stat_p_mean["statistic"] = "mean"
stat_p_max["statistic"] = "max"
stat_p_min["statistic"] = "min"
stat_p_std["statistic"] = "std"

recol = ["statistic", "role", "accuracy", "precision", "recall", "F1"]
stat_p_range = stat_p_range.reindex( columns=recol)
stat_p_mean = stat_p_mean.reindex( columns=recol)
stat_p_max = stat_p_max.reindex( columns=recol)
stat_p_min = stat_p_min.reindex( columns=recol)
stat_p_std = stat_p_std.reindex( columns=recol)

stat_p_diff = pd.concat( [stat_p_range, stat_p_mean, stat_p_max, stat_p_min, stat_p_std], axis=0 )

stat_p_diff



stat_input_mean = stat.groupby(["input"])[["accuracy", "precision", "recall", "F1"]].mean().reset_index()
stat_input_max  = stat.groupby(["input"])[["accuracy", "precision", "recall", "F1"]].max().reset_index()
stat_input_min  = stat.groupby(["input"])[["accuracy", "precision", "recall", "F1"]].min().reset_index()
stat_input_std  = stat.groupby(["input"])[["accuracy", "precision", "recall", "F1"]].std().reset_index()


stat_input_range = pd.DataFrame( columns=["input", "accuracy", "precision", "recall", "F1"])

for i in range(len(stat_input_max)):
  stat_input_range.loc[i,"input"] = stat_input_max.loc[i,"input"]
  stat_input_range.loc[i,"accuracy"] = stat_input_max.loc[i,"accuracy"] - stat_input_min.loc[i,"accuracy"]
  stat_input_range.loc[i,"precision"] = stat_input_max.loc[i,"precision"] - stat_input_min.loc[i,"precision"]
  stat_input_range.loc[i,"recall"] = stat_input_max.loc[i,"recall"] - stat_input_min.loc[i,"recall"]
  stat_input_range.loc[i,"F1"] = stat_input_max.loc[i,"F1"] - stat_input_min.loc[i,"F1"]

stat_input_range["statistic"] = "range"
stat_input_mean["statistic"] = "mean"
stat_input_max["statistic"] = "max"
stat_input_min["statistic"] = "min"
stat_input_std["statistic"] = "std"

recol = ["statistic", "input", "accuracy", "precision", "recall", "F1"]
stat_input_range = stat_input_range.reindex( columns=recol)
stat_input_mean  = stat_input_mean.reindex( columns=recol)
stat_input_max   = stat_input_max.reindex( columns=recol)
stat_input_min   = stat_input_min.reindex( columns=recol)
stat_input_std   = stat_input_std.reindex( columns=recol)

stat_input_diff = pd.concat( [stat_input_range, stat_input_mean, stat_input_max, stat_input_min, stat_input_std], axis=0 )

stat_input_diff



stat_output_mean = stat.groupby(["output"])[["accuracy", "precision", "recall", "F1"]].mean().reset_index()
stat_output_max  = stat.groupby(["output"])[["accuracy", "precision", "recall", "F1"]].max().reset_index()
stat_output_min  = stat.groupby(["output"])[["accuracy", "precision", "recall", "F1"]].min().reset_index()
stat_output_std  = stat.groupby(["output"])[["accuracy", "precision", "recall", "F1"]].std().reset_index()


stat_output_range = pd.DataFrame( columns=["output", "accuracy", "precision", "recall", "F1"])

for i in range(len(stat_input_max)):
  stat_output_range.loc[i,"output"]     = stat_output_max.loc[i,"output"]
  stat_output_range.loc[i,"accuracy"]  = stat_output_max.loc[i,"accuracy"]  - stat_output_min.loc[i,"accuracy"]
  stat_output_range.loc[i,"precision"] = stat_output_max.loc[i,"precision"] - stat_output_min.loc[i,"precision"]
  stat_output_range.loc[i,"recall"]    = stat_output_max.loc[i,"recall"]    - stat_output_min.loc[i,"recall"]
  stat_output_range.loc[i,"F1"]        = stat_output_max.loc[i,"F1"]        - stat_output_min.loc[i,"F1"]

stat_output_range["statistic"] = "range"
stat_output_mean["statistic"] = "mean"
stat_output_max["statistic"] = "max"
stat_output_min["statistic"] = "min"
stat_output_std["statistic"] = "std"

recol = ["statistic", "output", "accuracy", "precision", "recall", "F1"]
stat_output_range = stat_output_range.reindex( columns=recol)
stat_output_mean  = stat_output_mean.reindex( columns=recol)
stat_output_max   = stat_output_max.reindex( columns=recol)
stat_output_min   = stat_output_min.reindex( columns=recol)
stat_output_std   = stat_output_std.reindex( columns=recol)

stat_output_diff = pd.concat( [stat_output_range, stat_output_mean, stat_output_max, stat_output_min, stat_output_std], axis=0 )

stat_output_diff



stat_reputation_mean = stat.groupby(["reputation"])[["accuracy", "precision", "recall", "F1"]].mean().reset_index()
stat_reputation_max  = stat.groupby(["reputation"])[["accuracy", "precision", "recall", "F1"]].max().reset_index()
stat_reputation_min  = stat.groupby(["reputation"])[["accuracy", "precision", "recall", "F1"]].min().reset_index()
stat_reputation_std  = stat.groupby(["reputation"])[["accuracy", "precision", "recall", "F1"]].std().reset_index()


stat_reputation_range = pd.DataFrame( columns=["reputation", "accuracy", "precision", "recall", "F1"])

for i in range(len(stat_input_max)):
  stat_reputation_range.loc[i,"reputation"]    = stat_reputation_max.loc[i,"reputation"]
  stat_reputation_range.loc[i,"accuracy"]  = stat_reputation_max.loc[i,"accuracy"]  - stat_reputation_min.loc[i,"accuracy"]
  stat_reputation_range.loc[i,"precision"] = stat_reputation_max.loc[i,"precision"] - stat_reputation_min.loc[i,"precision"]
  stat_reputation_range.loc[i,"recall"]    = stat_reputation_max.loc[i,"recall"]    - stat_reputation_min.loc[i,"recall"]
  stat_reputation_range.loc[i,"F1"]        = stat_reputation_max.loc[i,"F1"]        - stat_reputation_min.loc[i,"F1"]

stat_reputation_range["statistic"] = "range"
stat_reputation_mean["statistic"] = "mean"
stat_reputation_max["statistic"] = "max"
stat_reputation_min["statistic"] = "min"
stat_reputation_std["statistic"] = "std"

recol = ["statistic", "reputation", "accuracy", "precision", "recall", "F1"]
stat_reputation_range = stat_reputation_range.reindex( columns=recol)
stat_reputation_mean  = stat_reputation_mean.reindex( columns=recol)
stat_reputation_max   = stat_reputation_max.reindex( columns=recol)
stat_reputation_min   = stat_reputation_min.reindex( columns=recol)
stat_reputation_std   = stat_reputation_std.reindex( columns=recol)

stat_reputation_diff = pd.concat( [stat_reputation_range, stat_reputation_mean, stat_reputation_max, stat_reputation_min, stat_reputation_std], axis=0 )

stat_reputation_diff



import pandas as pd

# 데이터 준비
data = [
    {'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1', 'manual_tagging_McNemar': 1.0837914423674379e-08, 'KR_FinBERT_McNemar': 0.00020457091514417203},
    {'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2', 'manual_tagging_McNemar': 2.7856391309572177e-13, 'KR_FinBERT_McNemar': 1.1524995281193037e-06},
    {'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3', 'manual_tagging_McNemar': 0.004024733785229442, 'KR_FinBERT_McNemar': 0.04516241775449287},
    {'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4', 'manual_tagging_McNemar': 2.4361955285077873e-18, 'KR_FinBERT_McNemar': 2.7646779844224555e-09},
    {'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1N', 'manual_tagging_McNemar': 6.404925733312152e-09, 'KR_FinBERT_McNemar': 4.009827338482897e-06},
    {'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2N', 'manual_tagging_McNemar': 3.814512488817119e-37, 'KR_FinBERT_McNemar': 2.8812609556977714e-24},
    {'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3N', 'manual_tagging_McNemar': 1.9973524596172616e-35, 'KR_FinBERT_McNemar': 1.1347966563854272e-23},
    {'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4N', 'manual_tagging_McNemar': 3.119997736245451e-09, 'KR_FinBERT_McNemar': 7.748910264239211e-06},
    {'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1C', 'manual_tagging_McNemar': 1.7738815010358873e-11, 'KR_FinBERT_McNemar': 4.768609724545183e-06},
    {'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2C', 'manual_tagging_McNemar': 1.4916559831780758e-16, 'KR_FinBERT_McNemar': 7.018264132797002e-09},
    {'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3C', 'manual_tagging_McNemar': 0.0011879303854797538, 'KR_FinBERT_McNemar': 0.018141890385553663},
    {'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4C', 'manual_tagging_McNemar': 7.157152153718532e-18, 'KR_FinBERT_McNemar': 2.5440063194724916e-09},
    {'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1NC', 'manual_tagging_McNemar': 1.758799640087663e-08, 'KR_FinBERT_McNemar': 1.1027271128514175e-06},
    {'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2NC', 'manual_tagging_McNemar': 5.499479320933321e-08, 'KR_FinBERT_McNemar': 0.0001728815996840573},
    {'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3NC', 'manual_tagging_McNemar': 2.837728337180794e-20, 'KR_FinBERT_McNemar': 1.2756223884637218e-11},
    {'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4NC', 'manual_tagging_McNemar': 0.2174455477829043, 'KR_FinBERT_McNemar': 0.33109394385577895},
    {'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'KR_FinBERT', 'manual_tagging_McNemar': 1.0, 'KR_FinBERT_McNemar': 1.0},
    {'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1', 'manual_tagging_McNemar': 0.0035448128470210523, 'KR_FinBERT_McNemar': 0.11803924678172628},
    {'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2', 'manual_tagging_McNemar': 1.0414019304757919e-06, 'KR_FinBERT_McNemar': 0.0029587002331942303},
    {'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3', 'manual_tagging_McNemar': 0.1420713794263756, 'KR_FinBERT_McNemar': 0.5485023614867373},
    {'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4', 'manual_tagging_McNemar': 2.612848003590018e-06, 'KR_FinBERT_McNemar': 0.005102556809829763},
    {'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1N', 'manual_tagging_McNemar': 0.005447429511444476, 'KR_FinBERT_McNemar': 0.06933702412999178},
    {'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2N', 'manual_tagging_McNemar': 3.0602379412652384e-15, 'KR_FinBERT_McNemar': 9.736635202617726e-09},
    {'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3N', 'manual_tagging_McNemar': 2.658284186005402e-17, 'KR_FinBERT_McNemar': 3.983187935376495e-10},
    {'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4N', 'manual_tagging_McNemar': 0.03192716208321237, 'KR_FinBERT_McNemar': 0.2613502966249506},
    {'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1C', 'manual_tagging_McNemar': 2.7188213670780667e-11, 'KR_FinBERT_McNemar': 4.3229892292211766e-05},
    {'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2C', 'manual_tagging_McNemar': 7.52058652903178e-15, 'KR_FinBERT_McNemar': 1.37097719610729e-07},
    {'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3C', 'manual_tagging_McNemar': 1.269697436840429e-06, 'KR_FinBERT_McNemar': 0.0027485762232244988},
    {'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4C', 'manual_tagging_McNemar': 2.120124047968334e-12, 'KR_FinBERT_McNemar': 3.103653771571732e-06},
    {'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1NC', 'manual_tagging_McNemar': 0.0002374680146274881, 'KR_FinBERT_McNemar': 0.0012060090535751763},
    {'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2NC', 'manual_tagging_McNemar': 0.0006361729964503618, 'KR_FinBERT_McNemar': 0.06002721569726564},
    {'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3NC', 'manual_tagging_McNemar': 1.0677540410372932e-60, 'KR_FinBERT_McNemar': 1.7248676975987686e-47},
    {'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4NC', 'manual_tagging_McNemar': 0.9121566075526022, 'KR_FinBERT_McNemar': 0.8724408026046263},
    {'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'KR_FinBERT', 'manual_tagging_McNemar': 0.7398815747182804, 'KR_FinBERT_McNemar': 1.0},
    {'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1', 'manual_tagging_McNemar': 0.007040750835507649, 'KR_FinBERT_McNemar': 0.09511486944573201},
    {'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2', 'manual_tagging_McNemar': 7.472488762210247e-11, 'KR_FinBERT_McNemar': 1.6494009300373766e-05},
    {'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3', 'manual_tagging_McNemar': 0.20795842151862862, 'KR_FinBERT_McNemar': 0.37307139305184334},
    {'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4', 'manual_tagging_McNemar': 1.432322294872245e-14, 'KR_FinBERT_McNemar': 1.4717241442903395e-07},
    {'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1N', 'manual_tagging_McNemar': 0.000553167687003418, 'KR_FinBERT_McNemar': 0.006380424956907065},
    {'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2N', 'manual_tagging_McNemar': 6.262566550385689e-44, 'KR_FinBERT_McNemar': 7.656621105782547e-30},
    {'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3N', 'manual_tagging_McNemar': 4.728263442876991e-47, 'KR_FinBERT_McNemar': 1.0230842464249794e-30},
    {'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4N', 'manual_tagging_McNemar': 1.5664730138788642e-24, 'KR_FinBERT_McNemar': 2.316059303376557e-16},
    {'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1C', 'manual_tagging_McNemar': 2.0707568742771897e-28, 'KR_FinBERT_McNemar': 5.91030320025199e-14},
    {'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2C', 'manual_tagging_McNemar': 1.8240209950594614e-32, 'KR_FinBERT_McNemar': 8.4389973396576435e-19},
    {'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3C', 'manual_tagging_McNemar': 1.7055073588391033e-13, 'KR_FinBERT_McNemar': 3.8297085821199745e-07},
    {'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4C', 'manual_tagging_McNemar': 4.417192007039092e-25, 'KR_FinBERT_McNemar': 1.0640356904972714e-13},
    {'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1NC', 'manual_tagging_McNemar': 1.509177966966307e-08, 'KR_FinBERT_McNemar': 2.1478912994713155e-06},
    {'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2NC', 'manual_tagging_McNemar': 4.870586356414979e-07, 'KR_FinBERT_McNemar': 0.0004525425812872116},
    {'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3NC', 'manual_tagging_McNemar': 7.44856535606066e-15, 'KR_FinBERT_McNemar': 5.410611980327476e-09},
    {'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4NC', 'manual_tagging_McNemar': 0.4655089982676935, 'KR_FinBERT_McNemar': 0.5406240719645112},
    {'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'KR_FinBERT', 'manual_tagging_McNemar': 0.9999999999999991, 'KR_FinBERT_McNemar': 1.0},
    {'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1', 'manual_tagging_McNemar': 0.5831321542611763, 'KR_FinBERT_McNemar': 0.7289185538924636},
    {'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2', 'manual_tagging_McNemar': 0.07217743850092258, 'KR_FinBERT_McNemar': 0.35558517636599585},
    {'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3', 'manual_tagging_McNemar': 0.6146550157654618, 'KR_FinBERT_McNemar': 0.8277787949824292},
    {'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4', 'manual_tagging_McNemar': 0.5966417603730826, 'KR_FinBERT_McNemar': 0.8295345705542331},
    {'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1N', 'manual_tagging_McNemar': 1.182808848491514e-05, 'KR_FinBERT_McNemar': 0.0027798977824854097},
    {'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2N', 'manual_tagging_McNemar': 0.050452410247217666, 'KR_FinBERT_McNemar': 0.19495973532361957},
    {'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3N', 'manual_tagging_McNemar': 0.00011112062020663294, 'KR_FinBERT_McNemar': 0.007203828622098756},
    {'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4N', 'manual_tagging_McNemar': 1.182808848491514e-05, 'KR_FinBERT_McNemar': 0.0027798977824854097},
    {'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1C', 'manual_tagging_McNemar': 0.4426260049464714, 'KR_FinBERT_McNemar': 0.6355297753805479},
    {'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2C', 'manual_tagging_McNemar': 1.0, 'KR_FinBERT_McNemar': 0.9999999999999991},
    {'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3C', 'manual_tagging_McNemar': 0.241644723140752, 'KR_FinBERT_McNemar': 0.4367534895513279},
    {'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4C', 'manual_tagging_McNemar': 0.0018204049356112858, 'KR_FinBERT_McNemar': 0.06343187528222856},
    {'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1NC', 'manual_tagging_McNemar': 0.007275502036308492, 'KR_FinBERT_McNemar': 0.0798030469373801},
    {'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2NC', 'manual_tagging_McNemar': 0.15067520762630385, 'KR_FinBERT_McNemar': 0.32099040312889304},
    {'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3NC', 'manual_tagging_McNemar': 0.0008775517800028651, 'KR_FinBERT_McNemar': 0.029724067777956706},
    {'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4NC', 'manual_tagging_McNemar': 3.552056811614334e-08, 'KR_FinBERT_McNemar': 0.000181229550274878},
    {'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'KR_FinBERT', 'manual_tagging_McNemar': 1.0, 'KR_FinBERT_McNemar': 1.0},
    {'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1', 'manual_tagging_McNemar': 0.49661743531793345, 'KR_FinBERT_McNemar': 0.5888916013592597},
    {'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2', 'manual_tagging_McNemar': 0.10478948242660333, 'KR_FinBERT_McNemar': 0.5307221711313294},
    {'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3', 'manual_tagging_McNemar': 1.0, 'KR_FinBERT_McNemar': 0.8878609477142523},
    {'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4', 'manual_tagging_McNemar': 0.6888797607233395, 'KR_FinBERT_McNemar': 0.6782220277695624},
    {'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1N', 'manual_tagging_McNemar': 9.375635307717017e-06, 'KR_FinBERT_McNemar': 0.002338660313927158},
    {'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2N', 'manual_tagging_McNemar': 0.028626655508737896, 'KR_FinBERT_McNemar': 0.12603420534178036},
    {'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3N', 'manual_tagging_McNemar': 3.2160015295666335e-05, 'KR_FinBERT_McNemar': 0.0022601352865399002},
    {'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4N', 'manual_tagging_McNemar': 9.375635307717017e-06, 'KR_FinBERT_McNemar': 0.002338660313927158},
    {'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1C', 'manual_tagging_McNemar': 0.19252645776146274, 'KR_FinBERT_McNemar': 0.38931368546615486},
    {'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2C', 'manual_tagging_McNemar': 0.47036853185814453, 'KR_FinBERT_McNemar': 0.5422534273379719},
    {'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3C', 'manual_tagging_McNemar': 0.022574972673257185, 'KR_FinBERT_McNemar': 0.307427498727949},
    {'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4C', 'manual_tagging_McNemar': 0.0025475648982539578, 'KR_FinBERT_McNemar': 0.13693395004631234},
    {'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1NC', 'manual_tagging_McNemar': 0.016508822028535212, 'KR_FinBERT_McNemar': 0.08573734055389333},
    {'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2NC', 'manual_tagging_McNemar': 1.373341332756231e-28, 'KR_FinBERT_McNemar': 9.457479873829774e-17},
    {'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3NC', 'manual_tagging_McNemar': 4.218899609200956e-29, 'KR_FinBERT_McNemar': 1.4433506959414592e-17},
    {'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4NC', 'manual_tagging_McNemar': 5.828029343883101e-67, 'KR_FinBERT_McNemar': 1.6865704463980894e-53},
    {'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'KR_FinBERT', 'manual_tagging_McNemar': 0.8962342571907211, 'KR_FinBERT_McNemar': 1.0},
    {'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1', 'manual_tagging_McNemar': 0.0001831165641297511, 'KR_FinBERT_McNemar': 0.12515357836207935},
    {'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2', 'manual_tagging_McNemar': 3.055970137442688e-14, 'KR_FinBERT_McNemar': 9.427186013028927e-06},
    {'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3', 'manual_tagging_McNemar': 7.050723457788823e-13, 'KR_FinBERT_McNemar': 4.0213286236766666e-05},
    {'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4', 'manual_tagging_McNemar': 6.216540579059133e-30, 'KR_FinBERT_McNemar': 2.7001249906849232e-14},
    {'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1N', 'manual_tagging_McNemar': 4.140210802639048e-171, 'KR_FinBERT_McNemar': 1.617269844780878e-173},
    {'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2N', 'manual_tagging_McNemar': 9.773558931737622e-68, 'KR_FinBERT_McNemar': 2.83058660056799e-54},
    {'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3N', 'manual_tagging_McNemar': 4.140210802639048e-171, 'KR_FinBERT_McNemar': 1.617269844780878e-173},
    {'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4N', 'manual_tagging_McNemar': 2.7133285516175262e-166, 'KR_FinBERT_McNemar': 4.97832626165811e-163},
    {'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1C', 'manual_tagging_McNemar': 0.02381979285800128, 'KR_FinBERT_McNemar': 0.467736334826734},
    {'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2C', 'manual_tagging_McNemar': 2.9201448769714557e-08, 'KR_FinBERT_McNemar': 0.006554322794320104},
    {'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3C', 'manual_tagging_McNemar': 0.012812402175831373, 'KR_FinBERT_McNemar': 0.3898213933200055},
    {'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4C', 'manual_tagging_McNemar': 0.0003783427631607787, 'KR_FinBERT_McNemar': 0.14174975029999165},
    {'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1NC', 'manual_tagging_McNemar': 9.558676266486855e-123, 'KR_FinBERT_McNemar': 7.39453638227656e-111},
    {'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2NC', 'manual_tagging_McNemar': 1.5380202394934788e-35, 'KR_FinBERT_McNemar': 5.992999561862181e-24},
    {'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3NC', 'manual_tagging_McNemar': 9.605791760977817e-42, 'KR_FinBERT_McNemar': 1.030294175464445e-28},
    {'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4NC', 'manual_tagging_McNemar': 4.292099113889264e-80, 'KR_FinBERT_McNemar': 6.5258606922603e-67},
    {'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'KR_FinBERT', 'manual_tagging_McNemar': 0.6768674798115425, 'KR_FinBERT_McNemar': 1.0},
]

# 데이터프레임 생성
df = pd.DataFrame(data)

# 데이터프레임 출력
# import ace_tools as tools; tools.display_dataframe_to_user(name="McNemar Test DataFrame", dataframe=df)

print(df)

df.loc[(df["manual_tagging_McNemar"]>= 0.05) & (df["prompt"]!="KR_FinBERT")].reset_index(drop=True)



data = [
    {'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1', 'manual_tagging_McNemar': 1.0837914423674379e-08, 'KR_FinBERT_McNemar': 0.00020457091514417203},
{'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2', 'manual_tagging_McNemar': 2.7856391309572177e-13, 'KR_FinBERT_McNemar': 1.1524995281193037e-06},
{'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3', 'manual_tagging_McNemar': 0.004024733785229442, 'KR_FinBERT_McNemar': 0.04516241775449287},
{'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4', 'manual_tagging_McNemar': 2.4361955285077873e-18, 'KR_FinBERT_McNemar': 2.7646779844224555e-09},
{'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1N', 'manual_tagging_McNemar': 6.404925733312152e-09, 'KR_FinBERT_McNemar': 4.009827338482897e-06},
{'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2N', 'manual_tagging_McNemar': 3.814512488817119e-37, 'KR_FinBERT_McNemar': 2.8812609556977714e-24},
{'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3N', 'manual_tagging_McNemar': 1.9973524596172616e-35, 'KR_FinBERT_McNemar': 1.1347966563854272e-23},
{'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4N', 'manual_tagging_McNemar': 3.119997736245451e-09, 'KR_FinBERT_McNemar': 7.748910264239211e-06},
{'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1C', 'manual_tagging_McNemar': 1.7738815010358873e-11, 'KR_FinBERT_McNemar': 4.768609724545183e-06},
{'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2C', 'manual_tagging_McNemar': 1.4916559831780758e-16, 'KR_FinBERT_McNemar': 7.018264132797002e-09},
{'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3C', 'manual_tagging_McNemar': 0.0011879303854797538, 'KR_FinBERT_McNemar': 0.018141890385553663},
{'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4C', 'manual_tagging_McNemar': 7.157152153718532e-18, 'KR_FinBERT_McNemar': 2.5440063194724916e-09},
{'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1NC', 'manual_tagging_McNemar': 1.758799640087663e-08, 'KR_FinBERT_McNemar': 1.1027271128514175e-06},
{'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2NC', 'manual_tagging_McNemar': 5.499479320933321e-08, 'KR_FinBERT_McNemar': 0.0001728815996840573},
{'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3NC', 'manual_tagging_McNemar': 2.837728337180794e-20, 'KR_FinBERT_McNemar': 1.2756223884637218e-11},
{'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4NC', 'manual_tagging_McNemar': 0.2174455477829043, 'KR_FinBERT_McNemar': 0.33109394385577895},
{'reputation': 1, 'engine': 'GPT-3.5-Turbo', 'prompt': 'KR_FinBERT', 'manual_tagging_McNemar': 1.0, 'KR_FinBERT_McNemar': 1.0},
{'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1', 'manual_tagging_McNemar': 0.0035448128470210523, 'KR_FinBERT_McNemar': 0.11803924678172628},
{'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2', 'manual_tagging_McNemar': 1.0414019304757919e-06, 'KR_FinBERT_McNemar': 0.0029587002331942303},
{'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3', 'manual_tagging_McNemar': 0.1420713794263756, 'KR_FinBERT_McNemar': 0.5485023614867373},
{'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4', 'manual_tagging_McNemar': 2.612848003590018e-06, 'KR_FinBERT_McNemar': 0.005102556809829763},
{'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1N', 'manual_tagging_McNemar': 0.005447429511444476, 'KR_FinBERT_McNemar': 0.06933702412999178},
{'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2N', 'manual_tagging_McNemar': 3.0602379412652384e-15, 'KR_FinBERT_McNemar': 9.736635202617726e-09},
{'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3N', 'manual_tagging_McNemar': 2.658284186005402e-17, 'KR_FinBERT_McNemar': 3.983187935376495e-10},
{'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4N', 'manual_tagging_McNemar': 0.03192716208321237, 'KR_FinBERT_McNemar': 0.2613502966249506},
{'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1C', 'manual_tagging_McNemar': 2.7188213670780667e-11, 'KR_FinBERT_McNemar': 4.3229892292211766e-05},
{'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2C', 'manual_tagging_McNemar': 7.52058652903178e-15, 'KR_FinBERT_McNemar': 1.37097719610729e-07},
{'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3C', 'manual_tagging_McNemar': 1.269697436840429e-06, 'KR_FinBERT_McNemar': 0.0027485762232244988},
{'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4C', 'manual_tagging_McNemar': 2.120124047968334e-12, 'KR_FinBERT_McNemar': 3.103653771571732e-06},
{'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1NC', 'manual_tagging_McNemar': 0.0002374680146274881, 'KR_FinBERT_McNemar': 0.0012060090535751763},
{'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2NC', 'manual_tagging_McNemar': 0.0006361729964503618, 'KR_FinBERT_McNemar': 0.06002721569726564},
{'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3NC', 'manual_tagging_McNemar': 1.0677540410372932e-60, 'KR_FinBERT_McNemar': 1.7248676975987686e-47},
{'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4NC', 'manual_tagging_McNemar': 0.9121566075526022, 'KR_FinBERT_McNemar': 0.8724408026046263},
{'reputation': 2, 'engine': 'GPT-3.5-Turbo', 'prompt': 'KR_FinBERT', 'manual_tagging_McNemar': 0.7398815747182804, 'KR_FinBERT_McNemar': 1.0},
{'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1', 'manual_tagging_McNemar': 0.007040750835507649, 'KR_FinBERT_McNemar': 0.09511486944573201},
{'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2', 'manual_tagging_McNemar': 7.472488762210247e-11, 'KR_FinBERT_McNemar': 1.6494009300373766e-05},
{'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3', 'manual_tagging_McNemar': 0.20795842151862862, 'KR_FinBERT_McNemar': 0.37307139305184334},
{'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4', 'manual_tagging_McNemar': 1.432322294872245e-14, 'KR_FinBERT_McNemar': 1.4717241442903395e-07},
{'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1N', 'manual_tagging_McNemar': 0.000553167687003418, 'KR_FinBERT_McNemar': 0.006380424956907065},
{'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2N', 'manual_tagging_McNemar': 6.262566550385689e-44, 'KR_FinBERT_McNemar': 7.656621105782547e-30},
{'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3N', 'manual_tagging_McNemar': 4.728263442876991e-47, 'KR_FinBERT_McNemar': 1.0230842464249794e-30},
{'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4N', 'manual_tagging_McNemar': 1.5664730138788642e-24, 'KR_FinBERT_McNemar': 2.316059303376557e-16},
{'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1C', 'manual_tagging_McNemar': 2.0707568742771897e-28, 'KR_FinBERT_McNemar': 5.91030320025199e-14},
{'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2C', 'manual_tagging_McNemar': 1.8240209950594614e-32, 'KR_FinBERT_McNemar': 8.4389973396576435e-19},
{'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3C', 'manual_tagging_McNemar': 1.7055073588391033e-13, 'KR_FinBERT_McNemar': 3.8297085821199745e-07},
{'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4C', 'manual_tagging_McNemar': 4.417192007039092e-25, 'KR_FinBERT_McNemar': 1.0640356904972714e-13},
{'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P1NC', 'manual_tagging_McNemar': 1.509177966966307e-08, 'KR_FinBERT_McNemar': 2.1478912994713155e-06},
{'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P2NC', 'manual_tagging_McNemar': 4.870586356414979e-07, 'KR_FinBERT_McNemar': 0.0004525425812872116},
{'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P3NC', 'manual_tagging_McNemar': 7.44856535606066e-15, 'KR_FinBERT_McNemar': 5.410611980327476e-09},
{'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'GPT_P4NC', 'manual_tagging_McNemar': 0.4655089982676935, 'KR_FinBERT_McNemar': 0.5406240719645112},
{'reputation': 3, 'engine': 'GPT-3.5-Turbo', 'prompt': 'KR_FinBERT', 'manual_tagging_McNemar': 0.9999999999999991, 'KR_FinBERT_McNemar': 1.0},
{'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1', 'manual_tagging_McNemar': 0.5831321542611763, 'KR_FinBERT_McNemar': 0.7289185538924636},
{'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2', 'manual_tagging_McNemar': 0.07217743850092258, 'KR_FinBERT_McNemar': 0.35558517636599585},
{'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3', 'manual_tagging_McNemar': 0.6146550157654618, 'KR_FinBERT_McNemar': 0.8277787949824292},
{'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4', 'manual_tagging_McNemar': 0.5966417603730826, 'KR_FinBERT_McNemar': 0.8295345705542331},
{'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1N', 'manual_tagging_McNemar': 1.182808848491514e-05, 'KR_FinBERT_McNemar': 0.0027798977824854097},
{'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2N', 'manual_tagging_McNemar': 0.050452410247217666, 'KR_FinBERT_McNemar': 0.19495973532361957},
{'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3N', 'manual_tagging_McNemar': 0.00011112062020663294, 'KR_FinBERT_McNemar': 0.007203828622098756},
{'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4N', 'manual_tagging_McNemar': 1.182808848491514e-05, 'KR_FinBERT_McNemar': 0.0027798977824854097},
{'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1C', 'manual_tagging_McNemar': 0.4426260049464714, 'KR_FinBERT_McNemar': 0.6355297753805479},
{'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2C', 'manual_tagging_McNemar': 1.0, 'KR_FinBERT_McNemar': 0.9999999999999991},
{'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3C', 'manual_tagging_McNemar': 0.241644723140752, 'KR_FinBERT_McNemar': 0.4367534895513279},
{'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4C', 'manual_tagging_McNemar': 0.0018204049356112858, 'KR_FinBERT_McNemar': 0.06343187528222856},
{'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1NC', 'manual_tagging_McNemar': 0.007275502036308492, 'KR_FinBERT_McNemar': 0.0798030469373801},
{'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2NC', 'manual_tagging_McNemar': 0.15067520762630385, 'KR_FinBERT_McNemar': 0.32099040312889304},
{'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3NC', 'manual_tagging_McNemar': 0.0008775517800028651, 'KR_FinBERT_McNemar': 0.029724067777956706},
{'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4NC', 'manual_tagging_McNemar': 3.552056811614334e-08, 'KR_FinBERT_McNemar': 0.000181229550274878},
{'reputation': 1, 'engine': 'GPT-4.0-Turbo', 'prompt': 'KR_FinBERT', 'manual_tagging_McNemar': 1.0, 'KR_FinBERT_McNemar': 1.0},
{'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1', 'manual_tagging_McNemar': 0.49661743531793345, 'KR_FinBERT_McNemar': 0.5888916013592597},
{'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2', 'manual_tagging_McNemar': 0.10478948242660333, 'KR_FinBERT_McNemar': 0.5307221711313294},
{'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3', 'manual_tagging_McNemar': 1.0, 'KR_FinBERT_McNemar': 0.8878609477142523},
{'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4', 'manual_tagging_McNemar': 0.6888797607233395, 'KR_FinBERT_McNemar': 0.6782220277695624},
{'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1N', 'manual_tagging_McNemar': 9.375635307717017e-06, 'KR_FinBERT_McNemar': 0.002338660313927158},
{'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2N', 'manual_tagging_McNemar': 0.028626655508737896, 'KR_FinBERT_McNemar': 0.12603420534178036},
{'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3N', 'manual_tagging_McNemar': 3.2160015295666335e-05, 'KR_FinBERT_McNemar': 0.0022601352865399002},
{'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4N', 'manual_tagging_McNemar': 9.375635307717017e-06, 'KR_FinBERT_McNemar': 0.002338660313927158},
{'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1C', 'manual_tagging_McNemar': 0.19252645776146274, 'KR_FinBERT_McNemar': 0.38931368546615486},
{'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2C', 'manual_tagging_McNemar': 0.47036853185814453, 'KR_FinBERT_McNemar': 0.5422534273379719},
{'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3C', 'manual_tagging_McNemar': 0.022574972673257185, 'KR_FinBERT_McNemar': 0.307427498727949},
{'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4C', 'manual_tagging_McNemar': 0.0025475648982539578, 'KR_FinBERT_McNemar': 0.13693395004631234},
{'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1NC', 'manual_tagging_McNemar': 0.016508822028535212, 'KR_FinBERT_McNemar': 0.08573734055389333},
{'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2NC', 'manual_tagging_McNemar': 1.373341332756231e-28, 'KR_FinBERT_McNemar': 9.457479873829774e-17},
{'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3NC', 'manual_tagging_McNemar': 4.218899609200956e-29, 'KR_FinBERT_McNemar': 1.4433506959414592e-17},
{'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4NC', 'manual_tagging_McNemar': 5.828029343883101e-67, 'KR_FinBERT_McNemar': 1.6865704463980894e-53},
{'reputation': 2, 'engine': 'GPT-4.0-Turbo', 'prompt': 'KR_FinBERT', 'manual_tagging_McNemar': 0.8962342571907211, 'KR_FinBERT_McNemar': 1.0},
{'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1', 'manual_tagging_McNemar': 0.0001831165641297511, 'KR_FinBERT_McNemar': 0.12515357836207935},
{'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2', 'manual_tagging_McNemar': 3.055970137442688e-14, 'KR_FinBERT_McNemar': 9.427186013028927e-06},
{'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3', 'manual_tagging_McNemar': 7.050723457788823e-13, 'KR_FinBERT_McNemar': 4.0213286236766666e-05},
{'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4', 'manual_tagging_McNemar': 6.216540579059133e-30, 'KR_FinBERT_McNemar': 2.7001249906849232e-14},
{'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1N', 'manual_tagging_McNemar': 4.140210802639048e-171, 'KR_FinBERT_McNemar': 1.617269844780878e-173},
{'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2N', 'manual_tagging_McNemar': 9.773558931737622e-68, 'KR_FinBERT_McNemar': 2.83058660056799e-54},
{'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3N', 'manual_tagging_McNemar': 4.140210802639048e-171, 'KR_FinBERT_McNemar': 1.617269844780878e-173},
{'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4N', 'manual_tagging_McNemar': 2.7133285516175262e-166, 'KR_FinBERT_McNemar': 4.97832626165811e-163},
{'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1C', 'manual_tagging_McNemar': 0.02381979285800128, 'KR_FinBERT_McNemar': 0.467736334826734},
{'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2C', 'manual_tagging_McNemar': 2.9201448769714557e-08, 'KR_FinBERT_McNemar': 0.006554322794320104},
{'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3C', 'manual_tagging_McNemar': 0.012812402175831373, 'KR_FinBERT_McNemar': 0.3898213933200055},
{'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4C', 'manual_tagging_McNemar': 0.0003783427631607787, 'KR_FinBERT_McNemar': 0.14174975029999165},
{'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P1NC', 'manual_tagging_McNemar': 9.558676266486855e-123, 'KR_FinBERT_McNemar': 7.39453638227656e-111},
{'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P2NC', 'manual_tagging_McNemar': 1.5380202394934788e-35, 'KR_FinBERT_McNemar': 5.992999561862181e-24},
{'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P3NC', 'manual_tagging_McNemar': 9.605791760977817e-42, 'KR_FinBERT_McNemar': 1.030294175464445e-28},
{'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'GPT_P4NC', 'manual_tagging_McNemar': 4.292099113889264e-80, 'KR_FinBERT_McNemar': 6.5258606922603e-67},
{'reputation': 3, 'engine': 'GPT-4.0-Turbo', 'prompt': 'KR_FinBERT', 'manual_tagging_McNemar': 0.6768674798115425, 'KR_FinBERT_McNemar': 1.0}
]

df = pd.DataFrame(data)

df.loc[(df["manual_tagging_McNemar"]>= 0.05) & (df["prompt"]!="KR_FinBERT")].reset_index(drop=True)





stat.head(102)

stat.describe()

stat.groupby(["engine", "reputation"])[["accuracy", "precision", "recall", "F1"]].min().reset_index()

stat.groupby(["engine", "reputation"])[["accuracy", "precision", "recall", "F1"]].max().reset_index()

stat.groupby(["engine", "reputation"])[["accuracy", "precision", "recall", "F1"]].mean().reset_index()

stat.groupby(["engine", "reputation"])[["accuracy", "precision", "recall", "F1"]].std().reset_index()

stat.groupby(["engine"])[["accuracy", "precision", "recall", "F1"]].min().reset_index()

stat.groupby(["engine"])[["accuracy", "precision", "recall", "F1"]].max().reset_index()

stat.groupby(["engine"])[["accuracy", "precision", "recall", "F1"]].mean().reset_index()

stat.groupby(["engine"])[["accuracy", "precision", "recall", "F1"]].std().reset_index()

stat.groupby(["engine", "input" ])[["accuracy", "precision", "recall", "F1"]].min().reset_index()

stat.groupby(["engine", "input" ])[["accuracy", "precision", "recall", "F1"]].max().reset_index()

stat.groupby(["engine", "input" ])[["accuracy", "precision", "recall", "F1"]].mean().reset_index()

stat.groupby(["engine", "input" ])[["accuracy", "precision", "recall", "F1"]].std().reset_index()

stat.groupby(["engine", "output" ])[["accuracy", "precision", "recall", "F1"]].min().reset_index()

stat.groupby(["engine", "output" ])[["accuracy", "precision", "recall", "F1"]].max().reset_index()

stat.groupby(["engine", "output" ])[["accuracy", "precision", "recall", "F1"]].mean().reset_index()

stat.groupby(["engine", "output" ])[["accuracy", "precision", "recall", "F1"]].std().reset_index()

stat.groupby(["engine", "prompt" ])[["accuracy", "precision", "recall", "F1"]].min().reset_index()

stat.groupby(["engine", "prompt" ])[["accuracy", "precision", "recall", "F1"]].max().reset_index()

stat.groupby(["engine", "prompt" ])[["accuracy", "precision", "recall", "F1"]].mean().reset_index()

stat.groupby(["engine", "prompt" ])[["accuracy", "precision", "recall", "F1"]].std().reset_index()

















stat_prompt = stat.groupby("prompt")[["accuracy", "precision", "recall", "F1"]].mean().reset_index()

stat_prompt

stat_reputation = stat.groupby(["engine", "reputation"])[["accuracy", "precision", "recall", "F1"]].mean().reset_index()

stat_reputation

stat_role = stat

stat_role["role"] = "-"

for i in range( len(stat_role) ):
  if stat_role.iloc[i, 2] == "GPT_P1" or stat_role.iloc[i, 2] == "GPT_P1C" or stat_role.iloc[i, 2] == "GPT_P1N" or stat_role.iloc[i, 2] == "GPT_P1NC":
    stat_role.iloc[i, -1] = "P1"
  elif stat_role.iloc[i, 2] == "GPT_P2" or stat_role.iloc[i, 2] == "GPT_P2C" or stat_role.iloc[i, 2] == "GPT_P2N" or stat_role.iloc[i, 2] == "GPT_P2NC":
    stat_role.iloc[i, -1] = "P2"
  elif stat_role.iloc[i, 2] == "GPT_P3" or stat_role.iloc[i, 2] == "GPT_P3C" or stat_role.iloc[i, 2] == "GPT_P3N" or stat_role.iloc[i, 2] == "GPT_P3NC":
    stat_role.iloc[i, -1] = "P3"
  elif stat_role.iloc[i, 2] == "GPT_P4" or stat_role.iloc[i, 2] == "GPT_P4C" or stat_role.iloc[i, 2] == "GPT_P4N" or stat_role.iloc[i, 2] == "GPT_P4NC":
    stat_role.iloc[i, -1] = "P4"

stat_role

stat_role.groupby("role")[["accuracy", "precision", "recall", "F1"]].mean()

stat_role.groupby("role")[["accuracy", "precision", "recall", "F1"]].max()

stat_role.groupby("role")[["accuracy", "precision", "recall", "F1"]].min()

stat_role.groupby("role")[["accuracy", "precision", "recall", "F1"]].max() - stat_role.groupby("role")[["accuracy", "precision", "recall", "F1"]].min()

stat_role.groupby("role")[["accuracy", "precision", "recall", "F1"]].std()

stat_output = stat



stat_output["output"] = "-"

for i in range( len(stat_role) ):
  if stat_output.iloc[i, 2] == "GPT_P1" or stat_output.iloc[i, 2] == "GPT_P2" or stat_output.iloc[i, 2] == "GPT_P3" or stat_output.iloc[i, 2] == "GPT_P4" or stat_output.iloc[i, 2] == "GPT_P1C" or stat_output.iloc[i, 2] == "GPT_P2C" or stat_output.iloc[i, 2] == "GPT_P3C" or stat_output.iloc[i, 2] == "GPT_P4C":
    stat_output.iloc[i, -1] = "binary"
  elif stat_output.iloc[i, 2] == "GPT_P1N" or stat_output.iloc[i, 2] == "GPT_P2N" or stat_output.iloc[i, 2] == "GPT_P3N" or stat_output.iloc[i, 2] == "GPT_P4N" or stat_output.iloc[i, 2] == "GPT_P1NC" or stat_output.iloc[i, 2] == "GPT_P2NC" or stat_output.iloc[i, 2] == "GPT_P3NC" or stat_output.iloc[i, 2] == "GPT_P4NC":
    stat_output.iloc[i, -1] = "numerical"

stat_output.groupby("input")[["accuracy", "precision", "recall", "F1"]].min()

stat_input = stat



stat_input["input"] = "-"

for i in range( len(stat_role) ):
  if stat_input.iloc[i, 2] == "GPT_P1" or stat_input.iloc[i, 2] == "GPT_P2" or stat_input.iloc[i, 2] == "GPT_P3" or stat_input.iloc[i, 2] == "GPT_P4" or stat_input.iloc[i, 2] == "GPT_P1N" or stat_input.iloc[i, 2] == "GPT_P2N" or stat_input.iloc[i, 2] == "GPT_P3N" or stat_input.iloc[i, 2] == "GPT_P4N":
    stat_input.iloc[i, -1] = "headline"
  elif stat_input.iloc[i, 2] == "GPT_P1C" or stat_input.iloc[i, 2] == "GPT_P2C" or stat_input.iloc[i, 2] == "GPT_P3C" or stat_input.iloc[i, 2] == "GPT_P4C" or stat_input.iloc[i, 2] == "GPT_P1NC" or stat_input.iloc[i, 2] == "GPT_P2NC" or stat_input.iloc[i, 2] == "GPT_P3NC" or stat_input.iloc[i, 2] == "GPT_P4NC":
    stat_input.iloc[i, -1] = "headline + content"

stat_input.groupby("output")[["accuracy", "precision", "recall", "F1"]].max() - stat_input.groupby("output")[["accuracy", "precision", "recall", "F1"]].min()

stat_prompt.describe()









stat_GPT40_c2

stat.to_csv( "/content/drive/MyDrive/Colab Notebooks/Paper/wide_news/stat.csv", index=False )

stat

stat.loc[(stat["prompt"]=="GPT_P1") & (stat["engine"]=="GPT-3.5-Turbo"), "accuracy"]

from scipy.stats import ttest_ind

def perform_ttest(metric_A, metric_B, metric_name):
    t_stat, p_value = ttest_ind(metric_A, metric_B)

    print( metric_A, metric_B )

    print(f"{metric_name}의 t-검정 결과:")
    print(f"t-통계량: {t_stat}")
    print(f"p-값: {p_value}\n")
    if p_value < 0.05:
        print(f"귀무가설 기각: {metric_name}에서 두 모형의 성능이 통계적으로 유의하게 다릅니다.\n")
    else:
        print(f"귀무가설 채택: {metric_name}에서 두 모형의 성능은 통계적으로 유의한 차이가 없습니다.\n")


t1 = stat.loc[(stat["prompt"]=="GPT_P1"), "accuracy"]
t2 = stat.loc[(stat["prompt"]=="KR_FinBERT"), "accuracy"]



# 각 성능 지표에 대해 t-검정 수행
perform_ttest(t1, t2, "정확도 (Accuracy)")
# perform_ttest(precision_A, precision_B, "정밀도 (Precision)")
# perform_ttest(recall_A, recall_B, "민감도 (Recall)")
# perform_ttest(f1_score_A, f1_score_B, "F1-Score")









"""### FinBERT Comparision

"""

stat_FinBERT = pd.DataFrame( columns=["prompt", "accuracy", "precision", "recall", "F1"] )

GPT_P1 = scoring( "GPT_P1", result_subset["KR_FinBERT"], result_subset["GPT_P1"] )
GPT_P2 = scoring( "GPT_P2", result_subset["KR_FinBERT"], result_subset["GPT_P2"] )
GPT_P3 = scoring( "GPT_P3", result_subset["KR_FinBERT"], result_subset["GPT_P3"] )
GPT_P4 = scoring( "GPT_P4", result_subset["KR_FinBERT"], result_subset["GPT_P4"] )
GPT_P1N = scoring( "GPT_P1N", result_subset["KR_FinBERT"], result_subset["GPT_P1N_str"] )
GPT_P2N = scoring( "GPT_P2N", result_subset["KR_FinBERT"], result_subset["GPT_P2N_str"] )
GPT_P3N = scoring( "GPT_P3N", result_subset["KR_FinBERT"], result_subset["GPT_P3N_str"] )
GPT_P4N = scoring( "GPT_P4N", result_subset["KR_FinBERT"], result_subset["GPT_P4N_str"] )

stat_FinBERT.loc[len(stat_FinBERT)] = GPT_P1
stat_FinBERT.loc[len(stat_FinBERT)] = GPT_P2
stat_FinBERT.loc[len(stat_FinBERT)] = GPT_P3
stat_FinBERT.loc[len(stat_FinBERT)] = GPT_P4
stat_FinBERT.loc[len(stat_FinBERT)] = GPT_P1N
stat_FinBERT.loc[len(stat_FinBERT)] = GPT_P2N
stat_FinBERT.loc[len(stat_FinBERT)] = GPT_P3N
stat_FinBERT.loc[len(stat_FinBERT)] = GPT_P4N

stat_FinBERT

"""### S-MAE"""

S_MAE_P1N = abs(result_subset["positive"] - result_subset["GPT_P1N"]).sum() / len(result_subset)
S_MAE_P2N = abs(result_subset["positive"] - result_subset["GPT_P2N"]).sum() / len(result_subset)
S_MAE_P3N = abs(result_subset["positive"] - result_subset["GPT_P3N"]).sum() / len(result_subset)
S_MAE_P4N = abs(result_subset["positive"] - result_subset["GPT_P4N"]).sum() / len(result_subset)

S_MAE_P1N

S_MAE_P2N

S_MAE_P3N

S_MAE_P4N



from statsmodels.stats.contingency_tables import mcnemar

# 예제 데이터 (혼동 행렬)
#       모델 2 예측
#        Yes   No
# 모델 1  Yes   40   10
# 예측    No    5    45

# 컨팅전시 테이블
table = [[40, 10],
         [5, 45]]

# McNemar 검정 수행
result = mcnemar(table, exact=True)
print('McNemar test p-value:', result.pvalue)